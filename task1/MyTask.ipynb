{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list() takes at most 1 argument (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-49f288b5e672>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;31m# Переводит категории в числовое представление #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mColumnApplier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_stages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_column_stages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list() takes at most 1 argument (3 given)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import string\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.tree import Tree\n",
    "from nltk.util import LazyMap, LazyConcatenation\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.corpus.reader.api import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "TRAINSET_PATH = \"./factrueval_trainset.npz\"\n",
    "TESTSET_PATH = \"./factrueval_testset.npz\"\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self,\n",
    "                 column_types=None,\n",
    "                 context_len=2,\n",
    "                 language='ru',\n",
    "                 number_of_occurences=5,\n",
    "                 weight_percentage=0.9):\n",
    "\n",
    "        # Частота, ниже которой лейбл считается \"редким\" #\n",
    "        self.NUMBER_OF_OCCURENCES = number_of_occurences\n",
    "\n",
    "        # Процент веса признаков, который нужно оставить\n",
    "        self.WEIGHT_PERCENTAGE = weight_percentage  #\n",
    "\n",
    "        # Информация о подаваемых столбцах (может быть WORD, POS, CHUNK) #\n",
    "        self._column_types = column_types if column_types is not None else [\"WORD\"]\n",
    "\n",
    "        # Длина рассматриваемого контекста (context_len влево и context_len вправо) #\n",
    "        self._context_len = context_len\n",
    "\n",
    "        # Анализатор (для POS-тега и начальной формы) #\n",
    "        self._morph = pymorphy2.MorphAnalyzer()\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Язык датасета (определяет используемые модули) #\n",
    "        self._lang = language\n",
    "\n",
    "        # OneHotEncoder, хранится после FIT-а #\n",
    "        self._enc = None\n",
    "\n",
    "        # ColumnApplier, хранится после FIT-а #\n",
    "        self._multi_encoder = None\n",
    "\n",
    "        # Словари распознаваемых слов, хранятся после FIT-а #\n",
    "        self._counters = []\n",
    "\n",
    "        # Число столбцов в \"сырой\" матрице признаков #\n",
    "        self._number_of_columns = None\n",
    "\n",
    "        # Индексы столбцов признаков, оставленных после отсева #\n",
    "        self._columns_to_keep = None\n",
    "\n",
    "    def fit_transform(self, data, answers, path, clf=ExtraTreesClassifier()):\n",
    "\n",
    "        # Eсли данные сохранены - просто берем их из файла #\n",
    "        if os.path.exists(path):\n",
    "            sparse_features_list = self.load_sparse_csr(path)\n",
    "            return sparse_features_list\n",
    "\n",
    "        # Добавляем пустые \"слова\" в начало и конец (для контекста) #\n",
    "        data = [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)] + data\n",
    "        data = data + [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)]\n",
    "\n",
    "        # Находим индексы столбцов в переданных данных #\n",
    "        word_index = self._column_types.index(\"WORD\")\n",
    "        if \"POS\" in self._column_types:\n",
    "            pos_index = self._column_types.index(\"POS\")\n",
    "        else:\n",
    "            pos_index = None\n",
    "        if \"POS\" in self._column_types:\n",
    "            chunk_index = self._column_types.index(\"CHUNK\")\n",
    "        else:\n",
    "            chunk_index = None\n",
    "\n",
    "        # Список признаков (строка == набор признаков для слова из массива data) #\n",
    "        features_list = []\n",
    "\n",
    "        # Заполнение массива features_list \"сырыми\" данными (без отсева) #\n",
    "        for k in range(len(data) - 2 * self._context_len):\n",
    "            arr = []\n",
    "            i = k + self._context_len\n",
    "\n",
    "            if pos_index is not None:\n",
    "                pos_arr = [data[i][pos_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(data[i - j][pos_index])\n",
    "                    pos_arr.append(data[i + j][pos_index])\n",
    "            else:\n",
    "                pos_arr = [self.get_pos_tag(data[i][word_index])]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(self.get_pos_tag(data[i - j][word_index]))\n",
    "                    pos_arr.append(self.get_pos_tag(data[i + j][word_index]))\n",
    "            arr += pos_arr\n",
    "\n",
    "            if chunk_index is not None:\n",
    "                chunk_arr = [data[i][chunk_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    chunk_arr.append(data[i - j][chunk_index])\n",
    "                    chunk_arr.append(data[i + j][chunk_index])\n",
    "                arr += chunk_arr\n",
    "\n",
    "            capital_arr = [self.get_capital(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                capital_arr.append(self.get_capital(data[i - j][word_index]))\n",
    "                capital_arr.append(self.get_capital(data[i + j][word_index]))\n",
    "            arr += capital_arr\n",
    "\n",
    "            is_punct_arr = [self.get_is_punct(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_punct_arr.append(self.get_is_punct(data[i - j][word_index]))\n",
    "                is_punct_arr.append(self.get_is_punct(data[i + j][word_index]))\n",
    "            arr += is_punct_arr\n",
    "\n",
    "            is_number_arr = [self.get_is_number(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_number_arr.append(self.get_is_number(data[i - j][word_index]))\n",
    "                is_number_arr.append(self.get_is_number(data[i + j][word_index]))\n",
    "            arr += is_number_arr\n",
    "\n",
    "            initial_arr = [self.get_initial(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                initial_arr.append(self.get_initial(data[i - j][word_index]))\n",
    "                initial_arr.append(self.get_initial(data[i + j][word_index]))\n",
    "            arr += initial_arr\n",
    "\n",
    "            features_list.append(arr)\n",
    "\n",
    "        # Теперь это массив сырых признаков (в строковом представлении, без отсева) #\n",
    "        features_list = np.array([np.array(line) for line in features_list])\n",
    "\n",
    "        # Выкинем из этого массива классы, встретившиеся менее NUMBER_OF_OCCURENCES раз #\n",
    "        # Посчитаем частоту лейблов в столбце #\n",
    "        self._number_of_columns = features_list.shape[1]\n",
    "        for u in range(self._number_of_columns):\n",
    "            arr = features_list[:, u]\n",
    "            counter = Counter(arr)\n",
    "            self._counters.append(counter)\n",
    "\n",
    "        # Избавимся от редких лейблов (частота < NUMBER_OF_OCC) #\n",
    "        for y in range(len(features_list)):\n",
    "            for x in range(self._number_of_columns):\n",
    "                features_list[y][x] = self.get_feature(x, features_list[y][x])\n",
    "\n",
    "        # Оставшиеся признаки бинаризуем #\n",
    "        self._multi_encoder = ColumnApplier(\n",
    "            dict([(i, preprocessing.LabelEncoder()) for i in range(len(features_list[0]))]))\n",
    "        features_list = self._multi_encoder.fit(features_list, None).transform(features_list)\n",
    "        self._enc = preprocessing.OneHotEncoder(dtype=np.bool_, sparse=True)\n",
    "        self._enc.fit(features_list)\n",
    "        features_list = self._enc.transform(features_list)\n",
    "\n",
    "        # Избавляемся от неинформативных признаков (WEIGHT = WEIGHT_PERC * TOTAL_WEIGHT)#\n",
    "        clf.fit(features_list, answers)\n",
    "        features_importances = [(i, el) for i, el in enumerate(clf.feature_importances_)]\n",
    "\n",
    "        features_importances = sorted(features_importances, key=lambda el: -el[1])\n",
    "        current_weight = 0.0\n",
    "        self._columns_to_keep = []\n",
    "        for el in features_importances:\n",
    "            self._columns_to_keep.append(el[0])\n",
    "            current_weight += el[1]\n",
    "            if current_weight > self.WEIGHT_PERCENTAGE:\n",
    "                break\n",
    "\n",
    "        features_list = features_list[:, self._columns_to_keep]\n",
    "\n",
    "        # Сохраняем матрицу в файл #\n",
    "        self.save_sparse_csr(path, features_list)\n",
    "\n",
    "        # Возвращаем матрицу #\n",
    "        return features_list\n",
    "\n",
    "    def transform(self, data, path):\n",
    "\n",
    "        # Eсли данные сохранены - просто берем их из файла #\n",
    "        if os.path.exists(path):\n",
    "            sparse_features_list = self.load_sparse_csr(path)\n",
    "            return sparse_features_list\n",
    "\n",
    "        # Добавляем пустые \"слова\" в начало и конец (для контекста) #\n",
    "        data = [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)] + data\n",
    "        data = data + [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)]\n",
    "\n",
    "        # Находим индексы столбцов в переданных данных #\n",
    "        word_index = self._column_types.index(\"WORD\")\n",
    "        if \"POS\" in self._column_types:\n",
    "            pos_index = self._column_types.index(\"POS\")\n",
    "        else:\n",
    "            pos_index = None\n",
    "        if \"CHUNK\" in self._column_types:\n",
    "            chunk_index = self._column_types.index(\"CHUNK\")\n",
    "        else:\n",
    "            chunk_index = None\n",
    "\n",
    "        # Список признаков (строка == набор признаков для слова из массива data) #\n",
    "        features_list = []\n",
    "\n",
    "        # Заполнение массива features_list \"сырыми\" данными (без отсева) #\n",
    "        for k in range(len(data) - 2 * self._context_len):\n",
    "            arr = []\n",
    "            i = k + self._context_len\n",
    "\n",
    "            if pos_index is not None:\n",
    "                pos_arr = [data[i][pos_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(data[i - j][pos_index])\n",
    "                    pos_arr.append(data[i + j][pos_index])\n",
    "            else:\n",
    "                pos_arr = [self.get_pos_tag(data[i][word_index])]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(self.get_pos_tag(data[i - j][word_index]))\n",
    "                    pos_arr.append(self.get_pos_tag(data[i + j][word_index]))\n",
    "            arr += pos_arr\n",
    "\n",
    "            if chunk_index is not None:\n",
    "                chunk_arr = [data[i][chunk_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    chunk_arr.append(data[i - j][chunk_index])\n",
    "                    chunk_arr.append(data[i + j][chunk_index])\n",
    "                arr += chunk_arr\n",
    "\n",
    "            capital_arr = [self.get_capital(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                capital_arr.append(self.get_capital(data[i - j][word_index]))\n",
    "                capital_arr.append(self.get_capital(data[i + j][word_index]))\n",
    "            arr += capital_arr\n",
    "\n",
    "            is_punct_arr = [self.get_is_punct(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_punct_arr.append(self.get_is_punct(data[i - j][word_index]))\n",
    "                is_punct_arr.append(self.get_is_punct(data[i + j][word_index]))\n",
    "            arr += is_punct_arr\n",
    "\n",
    "            is_number_arr = [self.get_is_number(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_number_arr.append(self.get_is_number(data[i - j][word_index]))\n",
    "                is_number_arr.append(self.get_is_number(data[i + j][word_index]))\n",
    "            arr += is_number_arr\n",
    "\n",
    "            initial_arr = [self.get_initial(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                initial_arr.append(self.get_initial(data[i - j][word_index]))\n",
    "                initial_arr.append(self.get_initial(data[i + j][word_index]))\n",
    "            arr += initial_arr\n",
    "\n",
    "            features_list.append(arr)\n",
    "\n",
    "        # Теперь это массив сырых признаков (в строковом представлении, без отсева) #\n",
    "        features_list = np.array([np.array(line) for line in features_list])\n",
    "\n",
    "        # Выкинем из этого массива классы, встретившиеся менее NUMBER_OF_OCCURENCES раз #\n",
    "        self._number_of_columns = features_list.shape[1]\n",
    "        for y in range(len(features_list)):\n",
    "            for x in range(self._number_of_columns):\n",
    "                features_list[y][x] = self.get_feature(x, features_list[y][x])\n",
    "\n",
    "        # Оставшиеся признаки бинаризуем #\n",
    "        features_list = self._multi_encoder.transform(features_list)\n",
    "        features_list = self._enc.transform(features_list)\n",
    "\n",
    "        # Избавляемся от неинформативных признаков (WEIGHT = WEIGHT_PERC * TOTAL_WEIGHT)#\n",
    "        features_list = features_list[:, self._columns_to_keep]\n",
    "\n",
    "        # Сохраняем матрицу в файл #\n",
    "        self.save_sparse_csr(path, features_list)\n",
    "\n",
    "        # Возвращаем матрицу #\n",
    "        return features_list\n",
    "\n",
    "    # Заменяет лейбл на \"*\", если он \"редкий\" #\n",
    "    def get_feature(self, f, feature):\n",
    "        if feature in self._counters[f].keys() and self._counters[f][feature] > self.NUMBER_OF_OCCURENCES:\n",
    "            return feature\n",
    "        else:\n",
    "            return \"*\"\n",
    "\n",
    "    # Сохраняет матрицу в файл #\n",
    "    def save_sparse_csr(self, filename, array):\n",
    "        np.savez(filename,\n",
    "                 data=array.data,\n",
    "                 indices=array.indices,\n",
    "                 indptr=array.indptr,\n",
    "                 shape=array.shape)\n",
    "\n",
    "    # Загружает матрицу из файла #\n",
    "    def load_sparse_csr(self, filename):\n",
    "        loader = np.load(filename)\n",
    "        return csr_matrix((loader['data'],\n",
    "                           loader['indices'],\n",
    "                           loader['indptr']),\n",
    "                          shape=loader['shape'])\n",
    "\n",
    "    # Возвращает POS-тег слова #\n",
    "    def get_pos_tag(self, token):\n",
    "        if self._lang == 'ru':\n",
    "            pos = self._morph.parse(token)[0].tag.POS\n",
    "        else:\n",
    "            pos = None\n",
    "        if pos is not None:\n",
    "            return pos\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    # Возвращает тип регистра слова #\n",
    "    def get_capital(self, token):\n",
    "        pattern = re.compile(\"[{}]+$\".format(re.escape(string.punctuation)))\n",
    "        if pattern.match(token):\n",
    "            return \"none\"\n",
    "        if len(token) == 0:\n",
    "            return \"none\"\n",
    "        if token.islower():\n",
    "            return \"lower\"\n",
    "        elif token.isupper():\n",
    "            return \"upper\"\n",
    "        elif token[0].isupper() and len(token) == 1:\n",
    "            return \"proper\"\n",
    "        elif token[0].isupper() and token[1:].islower():\n",
    "            return \"proper\"\n",
    "        else:\n",
    "            return \"camel\"\n",
    "\n",
    "    # Признак того, является ли слово числом #\n",
    "    def get_is_number(self, token):\n",
    "        try:\n",
    "            complex(token)\n",
    "        except ValueError:\n",
    "            return \"no\"\n",
    "        return \"yes\"\n",
    "\n",
    "    # Возвращает начальную форму слова #\n",
    "    def get_initial(self, token):\n",
    "        if self._lang == 'ru':\n",
    "            initial = self._morph.parse(token)[0].normal_form\n",
    "        else:\n",
    "            initial = self._lemmatizer.lemmatize(token)\n",
    "\n",
    "        if initial is not None:\n",
    "            return initial\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    # Признак того, является ли слово пунктуацией #\n",
    "    def get_is_punct(self, token):\n",
    "        pattern = re.compile(\"[{}]+$\".format(re.escape(string.punctuation)))\n",
    "        if pattern.match(token):\n",
    "            return \"yes\"\n",
    "        else:\n",
    "            return \"no\"\n",
    "\n",
    "\n",
    "# Переводит категории в числовое представление #\n",
    "class ColumnApplier(object):\n",
    "    def __init__(self, column_stages):\n",
    "        self._column_stages = column_stages\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for i, k in self._column_stages.items():\n",
    "            k.fit(x[:, i])\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        for i, k in self._column_stages.items():\n",
    "            x[:, i] = k.transform(x[:, i])\n",
    "        return x\n",
    "class ConllCorpusReaderX(CorpusReader):\n",
    "\n",
    "    WORDS = 'words'   #: column type for words\n",
    "    POS = 'pos'       #: column type for part-of-speech tags\n",
    "    TREE = 'tree'     #: column type for parse trees\n",
    "    CHUNK = 'chunk'   #: column type for chunk structures\n",
    "    NE = 'ne'         #: column type for named entities\n",
    "    SRL = 'srl'       #: column type for semantic role labels\n",
    "    IGNORE = 'ignore' #: column type for column that should be ignored\n",
    "    OFFSET = 'offset'\n",
    "    LEN = 'len'\n",
    "\n",
    "    #: A list of all column types supported by the conll corpus reader.\n",
    "    COLUMN_TYPES = (WORDS, POS, TREE, CHUNK, NE, SRL, IGNORE, OFFSET, LEN)\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Constructor\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "\n",
    "    def __init__(self, root, fileids, columntypes,\n",
    "                 chunk_types=None, root_label='S', pos_in_tree=False,\n",
    "                 srl_includes_roleset=True, encoding='utf8',\n",
    "                 tree_class=Tree, tagset=None):\n",
    "        for columntype in columntypes:\n",
    "            if columntype not in self.COLUMN_TYPES:\n",
    "                raise ValueError('Bad column type %r' % columntype)\n",
    "        if isinstance(chunk_types, string_types):\n",
    "            chunk_types = [chunk_types]\n",
    "        self._chunk_types = chunk_types\n",
    "        self._colmap = dict((c,i) for (i,c) in enumerate(columntypes))\n",
    "        self._pos_in_tree = pos_in_tree\n",
    "        self._root_label = root_label # for chunks\n",
    "        self._srl_includes_roleset = srl_includes_roleset\n",
    "        self._tree_class = tree_class\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        self._tagset = tagset\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        self._require(self.WORDS)\n",
    "        return LazyConcatenation(LazyMap(self._get_words, self._grids(fileids)))\n",
    "\n",
    "    def _grids(self, fileids=None):\n",
    "        # n.b.: we could cache the object returned here (keyed on\n",
    "        # fileids), which would let us reuse the same corpus view for\n",
    "        # different things (eg srl and parse trees).\n",
    "        return concat([StreamBackedCorpusView(fileid, self._read_grid_block,\n",
    "                                              encoding=enc)\n",
    "                       for (fileid, enc) in self.abspaths(fileids, True)])\n",
    "\n",
    "    def _read_grid_block(self, stream):\n",
    "        grids = []\n",
    "        for block in read_blankline_block(stream):\n",
    "            block = block.strip()\n",
    "            if not block: continue\n",
    "\n",
    "            grid = [line.split() for line in block.split('\\n')]\n",
    "\n",
    "            # If there's a docstart row, then discard. ([xx] eventually it\n",
    "            # would be good to actually use it)\n",
    "            if grid[0][self._colmap.get('words', 0)] == '-DOCSTART-':\n",
    "                del grid[0]\n",
    "\n",
    "            # Check that the grid is consistent.\n",
    "            for row in grid:\n",
    "                if len(row) != len(grid[0]):\n",
    "                    raise ValueError('Inconsistent number of columns:\\n%s'\n",
    "                                     % block)\n",
    "            grids.append(grid)\n",
    "        return grids\n",
    "\n",
    "    def get_ne(self, fileids=None, tagset=None):\n",
    "        self._require(self.NE)\n",
    "        def get_ne_inn(grid):\n",
    "            return self._get_ne(grid, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_ne_inn, self._grids(fileids)))\n",
    "\n",
    "    def _get_words(self, grid):\n",
    "        return self._get_column(grid, self._colmap['words'])\n",
    "\n",
    "    def _get_ne(self, grid, tagset=None):\n",
    "        return list(zip(self._get_column(grid, self._colmap['words']),\n",
    "                        self._get_column(grid, self._colmap['ne'])))\n",
    "\n",
    "    def _require(self, *columntypes):\n",
    "        for columntype in columntypes:\n",
    "            if columntype not in self._colmap:\n",
    "                raise ValueError('This corpus does not contain a %s '\n",
    "                                 'column.' % columntype)\n",
    "    @staticmethod\n",
    "    def _get_column(grid, column_index):\n",
    "        return [grid[i][column_index] for i in range(len(grid))]\n",
    "\n",
    "def prepare(dataset):\n",
    "    factrueval_dev_tokens = dict()\n",
    "    factrueval_dev_tokens_list = []\n",
    "    factrueval_dev_spans = dict()\n",
    "    factrueval_dev_objects = dict()\n",
    "    for file in os.listdir('./'+ dataset + '/'):\n",
    "        if file.endswith('tokens'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                lines = file_obj.readlines()\n",
    "                tokens = [line.rstrip().split() for line in lines if line.rstrip().split() != []]\n",
    "                for token in tokens:\n",
    "                    factrueval_dev_tokens[int(token[0])] = token[1:]\n",
    "                tokens = [line.rstrip().split() for line in lines]\n",
    "                for token in tokens:\n",
    "                    factrueval_dev_tokens_list.append(token)\n",
    "\n",
    "        if file.endswith('spans'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                spans = [line.rstrip().split() for line in file_obj.readlines() if line.rstrip().split() != []]\n",
    "                for span in spans:\n",
    "                    factrueval_dev_spans[span[0]] = span[1:]\n",
    "\n",
    "        if file.endswith('objects'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                objects = [line.rstrip().split('#')[0].split() for line in file_obj.readlines() if line.rstrip().split() != []]\n",
    "                for obj in objects:\n",
    "                    factrueval_dev_objects[obj[0]] = obj[1:]\n",
    "\n",
    "    all_ne = []\n",
    "    for key, value in factrueval_dev_objects.items():\n",
    "        spans = value[1:]\n",
    "        if value[0] == 'Location' or value[0] == 'Org' or value[0] == 'LocOrg':\n",
    "            ne = value[0]\n",
    "        else:\n",
    "            ne = 'O'\n",
    "        all_tokens = []\n",
    "        for span in spans:\n",
    "            span_obj = factrueval_dev_spans[span]\n",
    "            token = int(span_obj[3])\n",
    "            num_of_tokens = int(span_obj[4])\n",
    "            for i in range(num_of_tokens):\n",
    "                all_tokens.append(token + i)\n",
    "        all_ne.append([ne, sorted(all_tokens)])\n",
    "\n",
    "    for ne_tokens in all_ne:\n",
    "        ne = ne_tokens[0]\n",
    "        token = ne_tokens[1]\n",
    "        for i in range(len(token)):\n",
    "            if token[i] in factrueval_dev_tokens.keys():\n",
    "                if len(token) == 1:\n",
    "                    factrueval_dev_tokens[token[i]].append(\"S-\" + ne)\n",
    "                elif (i == 0 and token[i + 1] - token[i] > 1) or (i == len(token) - 1 and token[i] - token[i - 1] > 1) or (token[i] - token[i - 1] > 1 and token[i + 1] - token[i] > 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"S-\" + ne)\n",
    "                elif (i == 0  and token[i + 1] - token[i] == 1) or (i != len(token) - 1 and token[i] - token[i - 1] > 1 and token[i + 1] - token[i] == 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"B-\" + ne)\n",
    "                elif (i == len(token) - 1 and token[i] - token[i - 1] == 1) or (i != 0 and token[i] - token[i - 1] == 1 and token[i + 1] - token[i] > 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"E-\" + ne)\n",
    "                else:\n",
    "                    factrueval_dev_tokens[token[i]].append(\"I-\" + ne)\n",
    "\n",
    "    for i in range(len(factrueval_dev_tokens_list)):\n",
    "        if factrueval_dev_tokens_list[i] == []:\n",
    "            continue\n",
    "        number_of_token = factrueval_dev_tokens_list[i][0]\n",
    "        if int(number_of_token) in factrueval_dev_tokens.keys() and len(factrueval_dev_tokens[int(number_of_token)]) >= 4:\n",
    "            ne = factrueval_dev_tokens[int(number_of_token)][3]\n",
    "            factrueval_dev_tokens_list[i].append(ne)\n",
    "        else:\n",
    "            factrueval_dev_tokens_list[i].append(\"O\")\n",
    "\n",
    "    final = []\n",
    "    for el in factrueval_dev_tokens_list:\n",
    "        if el == []:\n",
    "            final.append(el)\n",
    "        else:\n",
    "            final.append([el[3], el[1], el[2], el[4]])\n",
    "    return final\n",
    "\n",
    "def dataSetFile(path, dataset):\n",
    "    with open(path, 'w+', encoding='utf-8') as file:\n",
    "        file.write(\"-DOCSTART- O\\n\")\n",
    "        for line in dataset:\n",
    "            if line == []:\n",
    "               file.write(\"\\n\")\n",
    "            else:\n",
    "                file.write(\"{} {} {} {}\\n\".format(*line))\n",
    "\n",
    "devset = prepare('devset')\n",
    "dataSetFile('./devset.txt', devset)\n",
    "testset = prepare('testset')\n",
    "dataSetFile('./testset.txt', testset)\n",
    "\n",
    "factrueval_devset = ConllCorpusReaderX('./', fileids='devset.txt', columntypes=['words', 'offset', 'len', 'ne'])\n",
    "factrueval_testset = ConllCorpusReaderX('./', fileids='testset.txt', columntypes=['words', 'offset', 'len', 'ne'])\n",
    "\n",
    "gen = Generator(column_types=['WORD'], context_len=2)\n",
    "\n",
    "Y_train = [el[1] for el in factrueval_devset.get_ne()]\n",
    "Y_test = [el[1] for el in factrueval_testset.get_ne()]\n",
    "\n",
    "X_train = gen.fit_transform([[el] for el in factrueval_devset.words()], Y_train, path=TRAINSET_PATH)\n",
    "X_test = gen.transform([[el] for el in factrueval_testset.words()], path=TESTSET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В\n",
      "понедельник\n",
      "28\n",
      "июня\n",
      "у\n",
      "здания\n",
      "мэрии\n",
      "Москвы\n",
      "на\n",
      "Тверской\n",
      "площади\n",
      "состоялась\n",
      "очередная\n",
      "несанкционированная\n",
      "акция\n",
      "протеста\n",
      "«\n",
      "День\n",
      "гнева\n",
      "»\n",
      ",\n",
      "в\n",
      "этот\n",
      "раз\n",
      "направленная\n",
      ",\n",
      "главным\n",
      "образом\n",
      ",\n",
      "против\n",
      "политики\n",
      "московских\n",
      "и\n",
      "подмосковных\n",
      "властей\n",
      ".\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-cec492579b00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdevset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in devset:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('test.csv', 'w', encoding='utf-8')as new_file:\n",
    "    csv_writer = csv.writer(new_file, delimiter=',')\n",
    "    for i in devset:\n",
    "        csv_writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
