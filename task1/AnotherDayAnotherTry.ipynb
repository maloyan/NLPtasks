{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymorphy2\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_PATH  = './devset/'\n",
    "TEST_PATH = './testset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_dev  = [f.split('.')[0] for f in os.listdir(DEV_PATH) if '.tokens' in f]\n",
    "filenames_test = [f.split('.')[0] for f in os.listdir(TEST_PATH) if '.tokens' in f]\n",
    "#filenames_dev = ['book_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(path = DEV_PATH, filenames = filenames_dev):\n",
    "    tokens = dict()\n",
    "    for file in filenames:\n",
    "        with open(path + file + '.tokens', 'r+', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                split = line.split()\n",
    "                if split:\n",
    "                    tokens[split[0]] = split[1:]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spans(path = DEV_PATH, filenames = filenames_dev):\n",
    "    spans = dict()\n",
    "    for file in filenames:\n",
    "        with open(path + file + '.spans', 'r+', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                split = line.split()\n",
    "                spans[split[0]] = split[1:]\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_objects(path = DEV_PATH, filenames = filenames_dev):\n",
    "    objects = dict()\n",
    "    for file in filenames:\n",
    "        with open(path + file + '.objects', 'r+', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                part = line.split(' # ')[0]\n",
    "                split = part.split()\n",
    "                if split[1] == 'Location' or split[1] == 'LocOrg':\n",
    "                    objects[split[0]] = split[1:]\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path = DEV_PATH, filenames = filenames_dev):\n",
    "    tokens = load_tokens(path, filenames)\n",
    "    spans  = load_spans(path, filenames)\n",
    "    objects = load_objects(path, filenames)\n",
    "    for key, value in objects.items():\n",
    "        ne = value[0]\n",
    "        span_ids = value[1:]\n",
    "        for i, span_id in enumerate(span_ids):\n",
    "            tokens[spans[span_ids[i]][3]].append(ne)\n",
    "    \n",
    "    token_list = []         \n",
    "    for key, value in tokens.items():\n",
    "        if len(value) == 3:\n",
    "            value.append('O')\n",
    "        token_list.append(value)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS  = 0\n",
    "LEN  = 1\n",
    "WORD = 2\n",
    "NE   = 3\n",
    "CTX_LEN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train = parse()\n",
    "feature_test = parse(TEST_PATH, filenames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS-тег слова #\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def get_pos(token):\n",
    "    pos = morph.parse(token)[0].tag.POS\n",
    "    if pos:\n",
    "        return pos\n",
    "    return \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тип регистра слова #\n",
    "def get_capital(token):\n",
    "    pattern = re.compile(\"[{}]+$\".format(re.escape(string.punctuation)))\n",
    "    if pattern.match(token):\n",
    "        return \"none\"\n",
    "    if len(token) == 0:\n",
    "        return \"none\"\n",
    "    if token.islower():\n",
    "        return \"lower\"\n",
    "    elif token.isupper():\n",
    "        return \"upper\"\n",
    "    elif token[0].isupper() and len(token) == 1:\n",
    "        return \"proper\"\n",
    "    elif token[0].isupper() and token[1:].islower():\n",
    "        return \"proper\"\n",
    "    else:\n",
    "        return \"camel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращает начальную форму слова #\n",
    "def get_initial(token):\n",
    "    init = morph.parse(token)[0].normal_form\n",
    "    if init:\n",
    "        return init\n",
    "    else:\n",
    "        return \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменяет лейбл на \"*\", если он \"редкий\" #\n",
    "NUMBER_OF_OCC = 5\n",
    "def get_feature(f, feature, counters):\n",
    "    if feature in counters[f].keys() and counters[f][feature] > NUMBER_OF_OCC:\n",
    "        return feature\n",
    "    else:\n",
    "        return \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводит категории в числовое представление #\n",
    "class ColumnApplier:\n",
    "    def __init__(self, column_stages):\n",
    "        self._column_stages = column_stages\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for i, k in self._column_stages.items():\n",
    "            k.fit(x[:, i])\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        for i, k in self._column_stages.items():\n",
    "            x[:, i] = k.transform(x[:, i])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выделим только слова\n",
    "def get_data(feature):\n",
    "    data = []\n",
    "    ans  = []\n",
    "    for f in feature:\n",
    "        data.append(f[WORD])\n",
    "        ans.append(f[NE])\n",
    "\n",
    "    # Добавим пустые слова для контекстной информации\n",
    "    data = [\"\" for i in range(CTX_LEN)] + data\n",
    "    data = data + [\"\" for i in range(CTX_LEN)]\n",
    "    return data, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_data(feature_train)\n",
    "X_test, Y_test = get_data(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_PERCENTAGE = 0.9\n",
    "columns_to_keep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data, ans, clf=ExtraTreesClassifier()):    \n",
    "    features_list = []\n",
    "    for k in range(len(data) - 2 * CTX_LEN):\n",
    "        arr = []\n",
    "        i = k + CTX_LEN\n",
    "\n",
    "        pos_arr = [get_pos(data[i])]\n",
    "        capital_arr = [get_capital(data[i])]\n",
    "        initial_arr = [get_initial(data[i])]\n",
    "\n",
    "        for j in range(1, CTX_LEN + 1):\n",
    "            pos_arr.append(get_pos(data[i - j]))\n",
    "            pos_arr.append(get_pos(data[i + j]))\n",
    "\n",
    "            capital_arr.append(get_capital(data[i - j]))\n",
    "            capital_arr.append(get_capital(data[i + j]))\n",
    "\n",
    "            initial_arr.append(get_initial(data[i - j]))\n",
    "            initial_arr.append(get_initial(data[i + j]))\n",
    "        \n",
    "        #arr += len(data[i])\n",
    "        arr += pos_arr\n",
    "        arr += capital_arr\n",
    "        arr += initial_arr\n",
    "\n",
    "        features_list.append(arr)\n",
    "    \n",
    "    features_list = np.array([np.array(line) for line in features_list])\n",
    "    \n",
    "    # Выкинем из этого массива классы, встретившиеся менее NUMBER_OF_OCCURENCES раз #\n",
    "    # Посчитаем частоту лейблов в столбце #\n",
    "    number_of_columns = features_list.shape[1]\n",
    "    counters = []\n",
    "    for u in range(number_of_columns):\n",
    "        arr = features_list[:, u]\n",
    "        counter = Counter(arr)\n",
    "        counters.append(counter)\n",
    "        \n",
    "    # Избавимся от редких лейблов (частота < NUMBER_OF_OCC) #\n",
    "    for y in range(len(features_list)):\n",
    "        for x in range(number_of_columns):\n",
    "            features_list[y][x] = get_feature(x, features_list[y][x], counters)\n",
    "            \n",
    "    multi_encoder = ColumnApplier(dict([(i, preprocessing.LabelEncoder()) for i in range(len(features_list[0]))]))\n",
    "    features_list = multi_encoder.fit(features_list, None).transform(features_list)\n",
    "    \n",
    "    enc = preprocessing.OneHotEncoder(dtype=np.bool_, sparse=True)\n",
    "    enc.fit(features_list)\n",
    "    features_list = enc.transform(features_list)\n",
    "    \n",
    "    clf.fit(features_list, ans)\n",
    "    features_importances = [(i, el) for i, el in enumerate(clf.feature_importances_)]\n",
    "    \n",
    "    \n",
    "    features_importances = sorted(features_importances, key=lambda el: -el[1])\n",
    "    current_weight = 0.0\n",
    "    \n",
    "    global columns_to_keep\n",
    "    if columns_to_keep == []:\n",
    "        for el in features_importances:\n",
    "            columns_to_keep.append(el[0])\n",
    "            current_weight += el[1]\n",
    "            if current_weight > WEIGHT_PERCENTAGE:\n",
    "                break\n",
    "\n",
    "    features_list = features_list[:, columns_to_keep]\n",
    "            \n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_features(X_train, Y_train)\n",
    "X_test = get_features(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_np = np.array(Y_test)\n",
    "predict_np = np.array(predict)\n",
    "Y_test_i = np.array([Y_test_np != 'O'])\n",
    "indexes = Y_test_i.reshape(Y_test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_fixed = Y_test_np[indexes]\n",
    "predict_fixed = predict_np[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_test_fixed, predict_fixed, average=\"weighted\", labels=['Location', 'LocOrg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Location', 'Location', 'Location', ..., 'Location', 'Location',\n",
       "       'Location'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_fixed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
