{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('scripts/dialent/task1/')\n",
    "sys.path.append('scripts/dialent/')\n",
    "sys.path.append('scripts/')\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narek\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\narek\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load the standard of book_3954:\n",
      "Unknown mention tag: Facility\n"
     ]
    }
   ],
   "source": [
    "u_dev = util.loadAllStandard('./devset/')\n",
    "u_test = util.loadAllStandard('./testset//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def get_pos(token):\n",
    "    pos = morph.parse(token)[0].tag.POS\n",
    "    if pos == 'NOUN':\n",
    "        return pos\n",
    "    return \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ne_bin(x):\n",
    "    if x == 'LOC':\n",
    "        return 0\n",
    "    elif x == 'LOCORG':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_bin(x):\n",
    "    if x == 'NOUN':\n",
    "        return 1\n",
    "    if x == 'ADJF':\n",
    "        return 2\n",
    "    if x == 'PRTF':\n",
    "        return 3\n",
    "    if x == 'VERB':\n",
    "        return 4\n",
    "    if x == 'CONJ':\n",
    "        return 5\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(u):\n",
    "    df = pd.DataFrame(columns=['tokens', 'tokens_id', 'start', 'end', 'ne'])\n",
    "    for i in range(len(u)):\n",
    "        res = u[i].makeTokenSets()\n",
    "        for j in range(len(res)):\n",
    "            token = res[j].toInlineString()\n",
    "            split = token.split()\n",
    "            df = df.append({'tokens': split[4:], 'first_token': str(split[4].split('\"')[1]), 'tokens_id': int(split[1]), 'start': int(split[2][1:-1]), \\\n",
    "               'end': int(split[3][:-1]), 'ne': str(split[0])}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = get_df(u_dev)\n",
    "df_dev = df_dev[(df_dev['ne'] == 'LOC') | (df_dev['ne'] == 'LOCORG')]\n",
    "df_dev['ne_bin'] = df_dev['ne'].apply(ne_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_df(u_test)\n",
    "df_test = df_test[(df_test['ne'] == 'LOC') | (df_test['ne'] == 'LOCORG')]\n",
    "df_test['ne_bin'] = df_test['ne'].apply(ne_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_dev.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char', #tokenizer=tokenize,\n",
    "               min_df=2, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "trn_term_doc = vec.fit_transform(df_dev['first_token'])\n",
    "test_term_doc = vec.transform(df_test['first_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dev['length'] = df_dev['end'] - df_dev['start'] + 1\n",
    "\n",
    "#df_dev['pos'] = df_dev['first_token'].apply(get_pos)\n",
    "#df_dev['pos_bin'] = df_dev['pos'].apply(pos_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dev = pd.concat([df_dev, pd.DataFrame(trn_term_doc.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['length']= df_test['end'] - df_test['start'] + 1\n",
    "\n",
    "#df_test['pos'] = df_test['first_token'].apply(get_pos)\n",
    "#df_test['pos_bin'] = df_test['pos'].apply(pos_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.concat([df_test, pd.DataFrame(test_term_doc.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = df_dev['ne_bin']\n",
    "Y_test = df_test['ne_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082, 1574)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_dev), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_dev = df_dev.drop(labels=['tokens', 'ne', 'ne_bin','first_token', 'pos'], axis=1)\n",
    "#X_test = df_test.drop(labels=['tokens', 'ne', 'ne_bin','first_token', 'pos'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\\n       silent=True, subsample=1)\\n\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "#brute force scan for all parameters, here are the tricks\n",
    "#usually max_depth is 6,7,8\n",
    "#learning rate is around 0.05, but small changes may make big diff\n",
    "#tuning min_child_weight subsample colsample_bytree can have \n",
    "#much fun of fighting against overfit \n",
    "#n_estimators is how many round of boosting\n",
    "#finally, ensemble xgboost with multiple seeds may reduce variance\n",
    "#parameters = {\n",
    "#    'max_depth':[4, 5, 6],\n",
    "#    'min_child_weight': [1, 2]\n",
    "#}\n",
    "\n",
    "#clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "#                   cv=StratifiedKFold(Y_dev, n_folds=2, shuffle=True), \n",
    "#                   #scoring='f1',\n",
    "#                   verbose=2, refit=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  10 out of  12 | elapsed:    3.6s remaining:    0.6s\n",
      "[Parallel(n_jobs=5)]: Done  12 out of  12 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.StratifiedKFold(labels=[1 1 ..., 0 1], n_folds=2, shuffle=True, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'max_depth': [4, 5, 6], 'min_child_weight': [1, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf.fit(trn_term_doc, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"= {\\n    'colsample_bytree': 0.4,\\n    'gamma': 1.1,\\n    'learning_rate': 0.1,\\n    'max_depth': 5,\\n    'min_child_weight': 1,\\n}\\n\""
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters = {\n",
    "    'colsample_bytree': 0.4,\n",
    "    'gamma': 1.1,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "'''= {\n",
    "    'colsample_bytree': 0.4,\n",
    "    'gamma': 1.1,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "}\n",
    "'''\n",
    "#0.903\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(**best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.4, gamma=1.1, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(trn_term_doc, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict(test_term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738881829733164\n"
     ]
    }
   ],
   "source": [
    "ans = np.array(Y_test)\n",
    "a = 0\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] == ans[i]:\n",
    "        a += 1\n",
    "print(a / len(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load the standard of book_3954:\n",
      "Unknown mention tag: Facility\n"
     ]
    }
   ],
   "source": [
    "u = util.loadAllStandard('./testset/')\n",
    "for i in range(len(u)):\n",
    "    res = u[i].makeTokenSets()\n",
    "    row = []\n",
    "    for j in range(len(res)):\n",
    "        token = res[j].toInlineString()\n",
    "        if token[0] == 'L':\n",
    "            #Заменить в этом ифе код на код предсказывания\n",
    "            split = token.split()\n",
    "            if predict[df_test['tokens_id'] == int(split[1])][0] == 0:\n",
    "                tag = 'LOC'\n",
    "            else:\n",
    "                tag = 'LOCORG'\n",
    "            start = int(split[2][1:-1])\n",
    "            end = int(split[3][:-1])\n",
    "            row.append('%s %d %d\\n' % (tag, start, end-start+1))\n",
    "        else:\n",
    "            split = token.split()\n",
    "            start = int(split[2][1:-1])\n",
    "            end = int(split[3][:-1])\n",
    "            row.append('%s %d %d\\n' % (split[0], start, end-start+1))\n",
    "    with open('./result/' + u[i].name + '.task1', 'w') as f:\n",
    "        f.writelines(row)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load the standard of book_3954:\n",
      "Unknown mention tag: Facility\n",
      "Type    P        R        F1       TP1      TP2      In Std.  In Test.\n",
      "per        0.9993   0.9993   0.9993  1342.00  1342.00     1343     1343\n",
      "loc        0.7146   0.6848   0.6994   409.49   409.49      598      573\n",
      "org        0.9895   0.9895   0.9895  1557.55  1557.55     1574     1574\n",
      "locorg     0.6714   0.8136   0.7357   515.00   515.00      633      767\n",
      "overall    0.8981   0.9218   0.9098  3817.04  3817.04     4141     4250\n"
     ]
    }
   ],
   "source": [
    "!python scripts/t1_eval.py -s ./testset -t ./result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
