{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINSET_PATH = \"./factrueval_trainset.npz\"\n",
    "TESTSET_PATH = \"./factrueval_testset.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Какие-то классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Generator:\n",
    "\n",
    "    def __init__(self,\n",
    "                 column_types=None,\n",
    "                 context_len=2,\n",
    "                 language='ru',\n",
    "                 number_of_occurences=5,\n",
    "                 weight_percentage=0.9):\n",
    "\n",
    "        # Частота, ниже которой лейбл считается \"редким\" #\n",
    "        self.NUMBER_OF_OCCURENCES = number_of_occurences\n",
    "\n",
    "        # Процент веса признаков, который нужно оставить\n",
    "        self.WEIGHT_PERCENTAGE = weight_percentage  #\n",
    "\n",
    "        # Информация о подаваемых столбцах (может быть WORD, POS, CHUNK) #\n",
    "        self._column_types = column_types if column_types is not None else [\"WORD\"]\n",
    "\n",
    "        # Длина рассматриваемого контекста (context_len влево и context_len вправо) #\n",
    "        self._context_len = context_len\n",
    "\n",
    "        # Анализатор (для POS-тега и начальной формы) #\n",
    "        self._morph = pymorphy2.MorphAnalyzer()\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Язык датасета (определяет используемые модули) #\n",
    "        self._lang = language\n",
    "\n",
    "        # OneHotEncoder, хранится после FIT-а #\n",
    "        self._enc = None\n",
    "\n",
    "        # ColumnApplier, хранится после FIT-а #\n",
    "        self._multi_encoder = None\n",
    "\n",
    "        # Словари распознаваемых слов, хранятся после FIT-а #\n",
    "        self._counters = []\n",
    "\n",
    "        # Число столбцов в \"сырой\" матрице признаков #\n",
    "        self._number_of_columns = None\n",
    "\n",
    "        # Индексы столбцов признаков, оставленных после отсева #\n",
    "        self._columns_to_keep = None\n",
    "\n",
    "    def fit_transform(self, data, answers, path, clf=ExtraTreesClassifier()):\n",
    "\n",
    "        # Eсли данные сохранены - просто берем их из файла #\n",
    "        if os.path.exists(path):\n",
    "            sparse_features_list = self.load_sparse_csr(path)\n",
    "            return sparse_features_list\n",
    "\n",
    "        # Добавляем пустые \"слова\" в начало и конец (для контекста) #\n",
    "        data = [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)] + data\n",
    "        data = data + [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)]\n",
    "\n",
    "        # Находим индексы столбцов в переданных данных #\n",
    "        word_index = self._column_types.index(\"WORD\")\n",
    "        if \"POS\" in self._column_types:\n",
    "            pos_index = self._column_types.index(\"POS\")\n",
    "        else:\n",
    "            pos_index = None\n",
    "        if \"POS\" in self._column_types:\n",
    "            chunk_index = self._column_types.index(\"CHUNK\")\n",
    "        else:\n",
    "            chunk_index = None\n",
    "\n",
    "        # Список признаков (строка == набор признаков для слова из массива data) #\n",
    "        features_list = []\n",
    "\n",
    "        # Заполнение массива features_list \"сырыми\" данными (без отсева) #\n",
    "        for k in range(len(data) - 2 * self._context_len):\n",
    "            arr = []\n",
    "            i = k + self._context_len\n",
    "\n",
    "            if pos_index is not None:\n",
    "                pos_arr = [data[i][pos_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(data[i - j][pos_index])\n",
    "                    pos_arr.append(data[i + j][pos_index])\n",
    "            else:\n",
    "                pos_arr = [self.get_pos_tag(data[i][word_index])]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(self.get_pos_tag(data[i - j][word_index]))\n",
    "                    pos_arr.append(self.get_pos_tag(data[i + j][word_index]))\n",
    "            arr += pos_arr\n",
    "\n",
    "            if chunk_index is not None:\n",
    "                chunk_arr = [data[i][chunk_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    chunk_arr.append(data[i - j][chunk_index])\n",
    "                    chunk_arr.append(data[i + j][chunk_index])\n",
    "                arr += chunk_arr\n",
    "\n",
    "            capital_arr = [self.get_capital(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                capital_arr.append(self.get_capital(data[i - j][word_index]))\n",
    "                capital_arr.append(self.get_capital(data[i + j][word_index]))\n",
    "            arr += capital_arr\n",
    "\n",
    "            is_punct_arr = [self.get_is_punct(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_punct_arr.append(self.get_is_punct(data[i - j][word_index]))\n",
    "                is_punct_arr.append(self.get_is_punct(data[i + j][word_index]))\n",
    "            arr += is_punct_arr\n",
    "\n",
    "            is_number_arr = [self.get_is_number(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_number_arr.append(self.get_is_number(data[i - j][word_index]))\n",
    "                is_number_arr.append(self.get_is_number(data[i + j][word_index]))\n",
    "            arr += is_number_arr\n",
    "\n",
    "            initial_arr = [self.get_initial(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                initial_arr.append(self.get_initial(data[i - j][word_index]))\n",
    "                initial_arr.append(self.get_initial(data[i + j][word_index]))\n",
    "            arr += initial_arr\n",
    "\n",
    "            features_list.append(arr)\n",
    "\n",
    "        # Теперь это массив сырых признаков (в строковом представлении, без отсева) #\n",
    "        features_list = np.array([np.array(line) for line in features_list])\n",
    "\n",
    "        # Выкинем из этого массива классы, встретившиеся менее NUMBER_OF_OCCURENCES раз #\n",
    "        # Посчитаем частоту лейблов в столбце #\n",
    "        self._number_of_columns = features_list.shape[1]\n",
    "        for u in range(self._number_of_columns):\n",
    "            arr = features_list[:, u]\n",
    "            counter = Counter(arr)\n",
    "            self._counters.append(counter)\n",
    "\n",
    "        # Избавимся от редких лейблов (частота < NUMBER_OF_OCC) #\n",
    "        for y in range(len(features_list)):\n",
    "            for x in range(self._number_of_columns):\n",
    "                features_list[y][x] = self.get_feature(x, features_list[y][x])\n",
    "        \n",
    "        # Оставшиеся признаки бинаризуем #\n",
    "        self._multi_encoder = ColumnApplier(\n",
    "            dict([(i, preprocessing.LabelEncoder()) for i in range(len(features_list[0]))]))\n",
    "        features_list = self._multi_encoder.fit(features_list, None).transform(features_list)\n",
    "        self._enc = preprocessing.OneHotEncoder(dtype=np.bool_, sparse=True)\n",
    "        self._enc.fit(features_list)\n",
    "        features_list = self._enc.transform(features_list)\n",
    "\n",
    "        # Избавляемся от неинформативных признаков (WEIGHT = WEIGHT_PERC * TOTAL_WEIGHT)#\n",
    "        clf.fit(features_list, answers)\n",
    "        features_importances = [(i, el) for i, el in enumerate(clf.feature_importances_)]\n",
    "\n",
    "        features_importances = sorted(features_importances, key=lambda el: -el[1])\n",
    "        current_weight = 0.0\n",
    "        self._columns_to_keep = []\n",
    "        for el in features_importances:\n",
    "            self._columns_to_keep.append(el[0])\n",
    "            current_weight += el[1]\n",
    "            if current_weight > self.WEIGHT_PERCENTAGE:\n",
    "                break\n",
    "\n",
    "        features_list = features_list[:, self._columns_to_keep]\n",
    "\n",
    "        # Сохраняем матрицу в файл #\n",
    "        self.save_sparse_csr(path, features_list)\n",
    "\n",
    "        # Возвращаем матрицу #\n",
    "        return features_list\n",
    "\n",
    "    def transform(self, data, path):\n",
    "\n",
    "        # Eсли данные сохранены - просто берем их из файла #\n",
    "        if os.path.exists(path):\n",
    "            sparse_features_list = self.load_sparse_csr(path)\n",
    "            return sparse_features_list\n",
    "\n",
    "        # Добавляем пустые \"слова\" в начало и конец (для контекста) #\n",
    "        data = [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)] + data\n",
    "        data = data + [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)]\n",
    "\n",
    "        # Находим индексы столбцов в переданных данных #\n",
    "        word_index = self._column_types.index(\"WORD\")\n",
    "        if \"POS\" in self._column_types:\n",
    "            pos_index = self._column_types.index(\"POS\")\n",
    "        else:\n",
    "            pos_index = None\n",
    "        if \"CHUNK\" in self._column_types:\n",
    "            chunk_index = self._column_types.index(\"CHUNK\")\n",
    "        else:\n",
    "            chunk_index = None\n",
    "\n",
    "        # Список признаков (строка == набор признаков для слова из массива data) #\n",
    "        features_list = []\n",
    "\n",
    "        # Заполнение массива features_list \"сырыми\" данными (без отсева) #\n",
    "        for k in range(len(data) - 2 * self._context_len):\n",
    "            arr = []\n",
    "            i = k + self._context_len\n",
    "\n",
    "            if pos_index is not None:\n",
    "                pos_arr = [data[i][pos_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(data[i - j][pos_index])\n",
    "                    pos_arr.append(data[i + j][pos_index])\n",
    "            else:\n",
    "                pos_arr = [self.get_pos_tag(data[i][word_index])]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(self.get_pos_tag(data[i - j][word_index]))\n",
    "                    pos_arr.append(self.get_pos_tag(data[i + j][word_index]))\n",
    "            arr += pos_arr\n",
    "\n",
    "            if chunk_index is not None:\n",
    "                chunk_arr = [data[i][chunk_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    chunk_arr.append(data[i - j][chunk_index])\n",
    "                    chunk_arr.append(data[i + j][chunk_index])\n",
    "                arr += chunk_arr\n",
    "\n",
    "            capital_arr = [self.get_capital(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                capital_arr.append(self.get_capital(data[i - j][word_index]))\n",
    "                capital_arr.append(self.get_capital(data[i + j][word_index]))\n",
    "            arr += capital_arr\n",
    "\n",
    "            is_punct_arr = [self.get_is_punct(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_punct_arr.append(self.get_is_punct(data[i - j][word_index]))\n",
    "                is_punct_arr.append(self.get_is_punct(data[i + j][word_index]))\n",
    "            arr += is_punct_arr\n",
    "\n",
    "            is_number_arr = [self.get_is_number(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_number_arr.append(self.get_is_number(data[i - j][word_index]))\n",
    "                is_number_arr.append(self.get_is_number(data[i + j][word_index]))\n",
    "            arr += is_number_arr\n",
    "\n",
    "            initial_arr = [self.get_initial(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                initial_arr.append(self.get_initial(data[i - j][word_index]))\n",
    "                initial_arr.append(self.get_initial(data[i + j][word_index]))\n",
    "            arr += initial_arr\n",
    "\n",
    "            features_list.append(arr)\n",
    "\n",
    "        # Теперь это массив сырых признаков (в строковом представлении, без отсева) #\n",
    "        features_list = np.array([np.array(line) for line in features_list])\n",
    "\n",
    "        # Выкинем из этого массива классы, встретившиеся менее NUMBER_OF_OCCURENCES раз #\n",
    "        self._number_of_columns = features_list.shape[1]\n",
    "        for y in range(len(features_list)):\n",
    "            for x in range(self._number_of_columns):\n",
    "                features_list[y][x] = self.get_feature(x, features_list[y][x])\n",
    "\n",
    "        # Оставшиеся признаки бинаризуем #\n",
    "        features_list = self._multi_encoder.transform(features_list)\n",
    "        features_list = self._enc.transform(features_list)\n",
    "\n",
    "        # Избавляемся от неинформативных признаков (WEIGHT = WEIGHT_PERC * TOTAL_WEIGHT)#\n",
    "        features_list = features_list[:, self._columns_to_keep]\n",
    "\n",
    "        # Сохраняем матрицу в файл #\n",
    "        self.save_sparse_csr(path, features_list)\n",
    "\n",
    "        # Возвращаем матрицу #\n",
    "        return features_list\n",
    "\n",
    "    # Заменяет лейбл на \"*\", если он \"редкий\" #\n",
    "    def get_feature(self, f, feature):\n",
    "        if feature in self._counters[f].keys() and self._counters[f][feature] > self.NUMBER_OF_OCCURENCES:\n",
    "            return feature\n",
    "        else:\n",
    "            return \"*\"\n",
    "\n",
    "    # Сохраняет матрицу в файл #\n",
    "    def save_sparse_csr(self, filename, array):\n",
    "        np.savez(filename,\n",
    "                 data=array.data,\n",
    "                 indices=array.indices,\n",
    "                 indptr=array.indptr,\n",
    "                 shape=array.shape)\n",
    "\n",
    "    # Загружает матрицу из файла #\n",
    "    def load_sparse_csr(self, filename):\n",
    "        loader = np.load(filename)\n",
    "        return csr_matrix((loader['data'],\n",
    "                           loader['indices'],\n",
    "                           loader['indptr']),\n",
    "                          shape=loader['shape'])\n",
    "\n",
    "    # Возвращает POS-тег слова #\n",
    "    def get_pos_tag(self, token):\n",
    "        if self._lang == 'ru':\n",
    "            pos = self._morph.parse(token)[0].tag.POS\n",
    "        else:\n",
    "            pos = None\n",
    "        if pos is not None:\n",
    "            return pos\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    # Возвращает тип регистра слова #\n",
    "    def get_capital(self, token):\n",
    "        pattern = re.compile(\"[{}]+$\".format(re.escape(string.punctuation)))\n",
    "        if pattern.match(token):\n",
    "            return \"none\"\n",
    "        if len(token) == 0:\n",
    "            return \"none\"\n",
    "        if token.islower():\n",
    "            return \"lower\"\n",
    "        elif token.isupper():\n",
    "            return \"upper\"\n",
    "        elif token[0].isupper() and len(token) == 1:\n",
    "            return \"proper\"\n",
    "        elif token[0].isupper() and token[1:].islower():\n",
    "            return \"proper\"\n",
    "        else:\n",
    "            return \"camel\"\n",
    "\n",
    "    # Признак того, является ли слово числом #\n",
    "    def get_is_number(self, token):\n",
    "        try:\n",
    "            complex(token)\n",
    "        except ValueError:\n",
    "            return \"no\"\n",
    "        return \"yes\"\n",
    "\n",
    "    # Возвращает начальную форму слова #\n",
    "    def get_initial(self, token):\n",
    "        if self._lang == 'ru':\n",
    "            initial = self._morph.parse(token)[0].normal_form\n",
    "        else:\n",
    "            initial = self._lemmatizer.lemmatize(token)\n",
    "\n",
    "        if initial is not None:\n",
    "            return initial\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    # Признак того, является ли слово пунктуацией #\n",
    "    def get_is_punct(self, token):\n",
    "        pattern = re.compile(\"[{}]+$\".format(re.escape(string.punctuation)))\n",
    "        if pattern.match(token):\n",
    "            return \"yes\"\n",
    "        else:\n",
    "            return \"no\"\n",
    "\n",
    "\n",
    "# Переводит категории в числовое представление #\n",
    "class ColumnApplier(object):\n",
    "    def __init__(self, column_stages):\n",
    "        self._column_stages = column_stages\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for i, k in self._column_stages.items():\n",
    "            k.fit(x[:, i])\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        for i, k in self._column_stages.items():\n",
    "            x[:, i] = k.transform(x[:, i])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import textwrap\n",
    "\n",
    "from six import string_types\n",
    "\n",
    "from nltk import compat\n",
    "from nltk.tree import Tree\n",
    "from nltk.util import LazyMap, LazyConcatenation\n",
    "from nltk.tag import map_tag\n",
    "\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.corpus.reader.api import *\n",
    "\n",
    "class ConllCorpusReaderX(CorpusReader):\n",
    "\n",
    "    WORDS = 'words'   #: column type for words\n",
    "    POS = 'pos'       #: column type for part-of-speech tags\n",
    "    TREE = 'tree'     #: column type for parse trees\n",
    "    CHUNK = 'chunk'   #: column type for chunk structures\n",
    "    NE = 'ne'         #: column type for named entities\n",
    "    SRL = 'srl'       #: column type for semantic role labels\n",
    "    IGNORE = 'ignore' #: column type for column that should be ignored\n",
    "    OFFSET = 'offset'\n",
    "    LEN = 'len'\n",
    "\n",
    "    #: A list of all column types supported by the conll corpus reader.\n",
    "    COLUMN_TYPES = (WORDS, POS, TREE, CHUNK, NE, SRL, IGNORE, OFFSET, LEN)\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Constructor\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "\n",
    "    def __init__(self, root, fileids, columntypes,\n",
    "                 chunk_types=None, root_label='S', pos_in_tree=False,\n",
    "                 srl_includes_roleset=True, encoding='utf8',\n",
    "                 tree_class=Tree, tagset=None):\n",
    "        for columntype in columntypes:\n",
    "            if columntype not in self.COLUMN_TYPES:\n",
    "                raise ValueError('Bad column type %r' % columntype)\n",
    "        if isinstance(chunk_types, string_types):\n",
    "            chunk_types = [chunk_types]\n",
    "        self._chunk_types = chunk_types\n",
    "        self._colmap = dict((c,i) for (i,c) in enumerate(columntypes))\n",
    "        self._pos_in_tree = pos_in_tree\n",
    "        self._root_label = root_label # for chunks\n",
    "        self._srl_includes_roleset = srl_includes_roleset\n",
    "        self._tree_class = tree_class\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        self._tagset = tagset\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Data Access Methods\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "\n",
    "    def raw(self, fileids=None):\n",
    "        if fileids is None: fileids = self._fileids\n",
    "        elif isinstance(fileids, string_types): fileids = [fileids]\n",
    "        return concat([self.open(f).read() for f in fileids])\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        self._require(self.WORDS)\n",
    "        return LazyConcatenation(LazyMap(self._get_words, self._grids(fileids)))\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        self._require(self.WORDS)\n",
    "        return LazyMap(self._get_words, self._grids(fileids))\n",
    "\n",
    "    def tagged_words(self, fileids=None, tagset=None):\n",
    "        self._require(self.WORDS, self.POS)\n",
    "        def get_tagged_words(grid):\n",
    "            return self._get_tagged_words(grid, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_tagged_words,\n",
    "                                         self._grids(fileids)))\n",
    "\n",
    "    def tagged_sents(self, fileids=None, tagset=None):\n",
    "        self._require(self.WORDS, self.POS)\n",
    "        def get_tagged_words(grid):\n",
    "            return self._get_tagged_words(grid, tagset)\n",
    "        return LazyMap(get_tagged_words, self._grids(fileids))\n",
    "\n",
    "    def chunked_words(self, fileids=None, chunk_types=None,\n",
    "                      tagset=None):\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        if chunk_types is None: chunk_types = self._chunk_types\n",
    "        def get_chunked_words(grid): # capture chunk_types as local var\n",
    "            return self._get_chunked_words(grid, chunk_types, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_chunked_words,\n",
    "                                         self._grids(fileids)))\n",
    "    \n",
    "    def get_tags(self, fileids=None, tagset=None, tags=[]):\n",
    "        required = []\n",
    "        for tag in tags:\n",
    "            if tag == 'offset':\n",
    "                required.append(self.OFFSET)\n",
    "            if tag == 'len':\n",
    "                required.append(self.LEN)\n",
    "            if tag == 'words':\n",
    "                required.append(self.WORDS)\n",
    "            if tag == 'pos':\n",
    "                required.append(self.POS)\n",
    "            if tag == 'tree':\n",
    "                required.append(self.TREE)\n",
    "            if tag == 'ne':\n",
    "                required.append(self.NE)\n",
    "            if tag == 'srl':\n",
    "                required.append(self.SRL)\n",
    "            if tag == 'ignore':\n",
    "                required.append(self.IGNORE)\n",
    "            if tag == 'chunk':\n",
    "                required.append(self.CHUNK)\n",
    "\n",
    "        self._require(*required)\n",
    "        def get_tags_inn(grid, tags=tags):\n",
    "            return self._get_tags(grid, tagset, tags=tags)\n",
    "        return LazyConcatenation(LazyMap(get_tags_inn, self._grids(fileids)))\n",
    "    \n",
    "    def _get_tags(self, grid, tagset=None, tags=None):\n",
    "        columns = [self._get_column(grid, self._colmap[tag]) for tag in tags]\n",
    "        return list(zip(*columns))\n",
    "    \n",
    "\n",
    "    def chunked_sents(self, fileids=None, chunk_types=None,\n",
    "                      tagset=None):\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        if chunk_types is None: chunk_types = self._chunk_types\n",
    "        def get_chunked_words(grid): # capture chunk_types as local var\n",
    "            return self._get_chunked_words(grid, chunk_types, tagset)\n",
    "        return LazyMap(get_chunked_words, self._grids(fileids))\n",
    "\n",
    "    def parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None):\n",
    "        self._require(self.WORDS, self.POS, self.TREE)\n",
    "        if pos_in_tree is None: pos_in_tree = self._pos_in_tree\n",
    "        def get_parsed_sent(grid): # capture pos_in_tree as local var\n",
    "            return self._get_parsed_sent(grid, pos_in_tree, tagset)\n",
    "        return LazyMap(get_parsed_sent, self._grids(fileids))\n",
    "\n",
    "    def srl_spans(self, fileids=None):\n",
    "        self._require(self.SRL)\n",
    "        return LazyMap(self._get_srl_spans, self._grids(fileids))\n",
    "\n",
    "    def srl_instances(self, fileids=None, pos_in_tree=None, flatten=True):\n",
    "        self._require(self.WORDS, self.POS, self.TREE, self.SRL)\n",
    "        if pos_in_tree is None: pos_in_tree = self._pos_in_tree\n",
    "        def get_srl_instances(grid): # capture pos_in_tree as local var\n",
    "            return self._get_srl_instances(grid, pos_in_tree)\n",
    "        result = LazyMap(get_srl_instances, self._grids(fileids))\n",
    "        if flatten: result = LazyConcatenation(result)\n",
    "        return result\n",
    "\n",
    "    def iob_words(self, fileids=None, tagset=None):\n",
    "        \"\"\"\n",
    "        :return: a list of word/tag/IOB tuples\n",
    "        :rtype: list(tuple)\n",
    "        :param fileids: the list of fileids that make up this corpus\n",
    "        :type fileids: None or str or list\n",
    "        \"\"\"\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        def get_iob_words(grid):\n",
    "            return self._get_iob_words(grid, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_iob_words, self._grids(fileids)))\n",
    "\n",
    "    def iob_sents(self, fileids=None, tagset=None):\n",
    "        \"\"\"\n",
    "        :return: a list of lists of word/tag/IOB tuples\n",
    "        :rtype: list(list)\n",
    "        :param fileids: the list of fileids that make up this corpus\n",
    "        :type fileids: None or str or list\n",
    "        \"\"\"\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        def get_iob_words(grid):\n",
    "            return self._get_iob_words(grid, tagset)\n",
    "        return LazyMap(get_iob_words, self._grids(fileids))\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Grid Reading\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "\n",
    "    def _grids(self, fileids=None):\n",
    "        # n.b.: we could cache the object returned here (keyed on\n",
    "        # fileids), which would let us reuse the same corpus view for\n",
    "        # different things (eg srl and parse trees).\n",
    "        return concat([StreamBackedCorpusView(fileid, self._read_grid_block,\n",
    "                                              encoding=enc)\n",
    "                       for (fileid, enc) in self.abspaths(fileids, True)])\n",
    "\n",
    "    def _read_grid_block(self, stream):\n",
    "        grids = []\n",
    "        for block in read_blankline_block(stream):\n",
    "            block = block.strip()\n",
    "            if not block: continue\n",
    "\n",
    "            grid = [line.split() for line in block.split('\\n')]\n",
    "\n",
    "            # If there's a docstart row, then discard. ([xx] eventually it\n",
    "            # would be good to actually use it)\n",
    "            if grid[0][self._colmap.get('words', 0)] == '-DOCSTART-':\n",
    "                del grid[0]\n",
    "\n",
    "            # Check that the grid is consistent.\n",
    "            for row in grid:\n",
    "                if len(row) != len(grid[0]):\n",
    "                    raise ValueError('Inconsistent number of columns:\\n%s'\n",
    "                                     % block)\n",
    "            grids.append(grid)\n",
    "        return grids\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Transforms\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # given a grid, transform it into some representation (e.g.,\n",
    "    # a list of words or a parse tree).\n",
    "\n",
    "    def _get_words(self, grid):\n",
    "        return self._get_column(grid, self._colmap['words'])\n",
    "\n",
    "    def _get_tagged_words(self, grid, tagset=None):\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags))\n",
    "\n",
    "    def _get_iob_words(self, grid, tagset=None):\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags,\n",
    "                   self._get_column(grid, self._colmap['chunk'])))\n",
    "\n",
    "    def _get_chunked_words(self, grid, chunk_types, tagset=None):\n",
    "        # n.b.: this method is very similar to conllstr2tree.\n",
    "        words = self._get_column(grid, self._colmap['words'])\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        chunk_tags = self._get_column(grid, self._colmap['chunk'])\n",
    "\n",
    "        stack = [Tree(self._root_label, [])]\n",
    "\n",
    "        for (word, pos_tag, chunk_tag) in zip(words, pos_tags, chunk_tags):\n",
    "            if chunk_tag == 'O':\n",
    "                state, chunk_type = 'O', ''\n",
    "            else:\n",
    "                (state, chunk_type) = chunk_tag.split('-')\n",
    "            # If it's a chunk we don't care about, treat it as O.\n",
    "            if chunk_types is not None and chunk_type not in chunk_types:\n",
    "                state = 'O'\n",
    "            # Treat a mismatching I like a B.\n",
    "            if state == 'I' and chunk_type != stack[-1].label():\n",
    "                state = 'B'\n",
    "            # For B or I: close any open chunks\n",
    "            if state in 'BO' and len(stack) == 2:\n",
    "                stack.pop()\n",
    "            # For B: start a new chunk.\n",
    "            if state == 'B':\n",
    "                new_chunk = Tree(chunk_type, [])\n",
    "                stack[-1].append(new_chunk)\n",
    "                stack.append(new_chunk)\n",
    "            # Add the word token.\n",
    "            stack[-1].append((word, pos_tag))\n",
    "\n",
    "        return stack[0]\n",
    "\n",
    "    def _get_parsed_sent(self, grid, pos_in_tree, tagset=None):\n",
    "        words = self._get_column(grid, self._colmap['words'])\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        parse_tags = self._get_column(grid, self._colmap['tree'])\n",
    "\n",
    "        treestr = ''\n",
    "        for (word, pos_tag, parse_tag) in zip(words, pos_tags, parse_tags):\n",
    "            if word == '(': word = '-LRB-'\n",
    "            if word == ')': word = '-RRB-'\n",
    "            if pos_tag == '(': pos_tag = '-LRB-'\n",
    "            if pos_tag == ')': pos_tag = '-RRB-'\n",
    "            (left, right) = parse_tag.split('*')\n",
    "            right = right.count(')')*')' # only keep ')'.\n",
    "            treestr += '%s (%s %s) %s' % (left, pos_tag, word, right)\n",
    "        try:\n",
    "            tree = self._tree_class.fromstring(treestr)\n",
    "        except (ValueError, IndexError):\n",
    "            tree = self._tree_class.fromstring('(%s %s)' %\n",
    "                                          (self._root_label, treestr))\n",
    "\n",
    "        if not pos_in_tree:\n",
    "            for subtree in tree.subtrees():\n",
    "                for i, child in enumerate(subtree):\n",
    "                    if (isinstance(child, Tree) and len(child)==1 and\n",
    "                        isinstance(child[0], string_types)):\n",
    "                        subtree[i] = (child[0], child.label())\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def _get_srl_spans(self, grid):\n",
    "        \"\"\"\n",
    "        list of list of (start, end), tag) tuples\n",
    "        \"\"\"\n",
    "        if self._srl_includes_roleset:\n",
    "            predicates = self._get_column(grid, self._colmap['srl']+1)\n",
    "            start_col = self._colmap['srl']+2\n",
    "        else:\n",
    "            predicates = self._get_column(grid, self._colmap['srl'])\n",
    "            start_col = self._colmap['srl']+1\n",
    "\n",
    "        # Count how many predicates there are.  This tells us how many\n",
    "        # columns to expect for SRL data.\n",
    "        num_preds = len([p for p in predicates if p != '-'])\n",
    "\n",
    "        spanlists = []\n",
    "        for i in range(num_preds):\n",
    "            col = self._get_column(grid, start_col+i)\n",
    "            spanlist = []\n",
    "            stack = []\n",
    "            for wordnum, srl_tag in enumerate(col):\n",
    "                (left, right) = srl_tag.split('*')\n",
    "                for tag in left.split('('):\n",
    "                    if tag:\n",
    "                        stack.append((tag, wordnum))\n",
    "                for i in range(right.count(')')):\n",
    "                    (tag, start) = stack.pop()\n",
    "                    spanlist.append( ((start, wordnum+1), tag) )\n",
    "            spanlists.append(spanlist)\n",
    "\n",
    "        return spanlists\n",
    "    \n",
    "    def get_ne(self, fileids=None, tagset=None):\n",
    "        self._require(self.NE)\n",
    "        def get_ne_inn(grid):\n",
    "            return self._get_ne(grid, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_ne_inn, self._grids(fileids)))\n",
    "    \n",
    "    def _get_ne(self, grid, tagset=None):\n",
    "        return list(zip(self._get_column(grid, self._colmap['words']),\n",
    "                        self._get_column(grid, self._colmap['ne'])))\n",
    "    \n",
    "    def _get_srl_instances(self, grid, pos_in_tree):\n",
    "        tree = self._get_parsed_sent(grid, pos_in_tree)\n",
    "        spanlists = self._get_srl_spans(grid)\n",
    "        if self._srl_includes_roleset:\n",
    "            predicates = self._get_column(grid, self._colmap['srl']+1)\n",
    "            rolesets = self._get_column(grid, self._colmap['srl'])\n",
    "        else:\n",
    "            predicates = self._get_column(grid, self._colmap['srl'])\n",
    "            rolesets = [None] * len(predicates)\n",
    "\n",
    "        instances = ConllSRLInstanceList(tree)\n",
    "        for wordnum, predicate in enumerate(predicates):\n",
    "            if predicate == '-': continue\n",
    "            for spanlist in spanlists:\n",
    "                for (start, end), tag in spanlist:\n",
    "                    if wordnum in range(start,end) and tag in ('V', 'C-V'):\n",
    "                        break\n",
    "                else: continue\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError('No srl column found for %r' % predicate)\n",
    "            instances.append(ConllSRLInstance(tree, wordnum, predicate,\n",
    "                                              rolesets[wordnum], spanlist))\n",
    "\n",
    "        return instances\n",
    "\n",
    "    def _require(self, *columntypes):\n",
    "        for columntype in columntypes:\n",
    "            if columntype not in self._colmap:\n",
    "                raise ValueError('This corpus does not contain a %s '\n",
    "                                 'column.' % columntype)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_column(grid, column_index):\n",
    "        return [grid[i][column_index] for i in range(len(grid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получаем devset.txt и testset.txt (converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def prepare(dataset):\n",
    "    factrueval_dev_tokens = dict()\n",
    "    factrueval_dev_tokens_list = []\n",
    "    factrueval_dev_spans = dict()\n",
    "    factrueval_dev_objects = dict()\n",
    "    for file in os.listdir('./'+ dataset + '/'):\n",
    "        if file.endswith('tokens'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                lines = file_obj.readlines()\n",
    "                tokens = [line.rstrip().split() for line in lines if line.rstrip().split() != []]\n",
    "                for token in tokens:\n",
    "                    factrueval_dev_tokens[int(token[0])] = token[1:]\n",
    "                tokens = [line.rstrip().split() for line in lines]\n",
    "                for token in tokens:\n",
    "                    factrueval_dev_tokens_list.append(token)\n",
    "\n",
    "        if file.endswith('spans'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                spans = [line.rstrip().split() for line in file_obj.readlines() if line.rstrip().split() != []]\n",
    "                for span in spans:\n",
    "                    factrueval_dev_spans[span[0]] = span[1:]\n",
    "\n",
    "        if file.endswith('objects'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                objects = [line.rstrip().split('#')[0].split() for line in file_obj.readlines() if line.rstrip().split() != []]\n",
    "                for obj in objects:\n",
    "                    factrueval_dev_objects[obj[0]] = obj[1:]\n",
    "\n",
    "    all_ne = []\n",
    "    for key, value in factrueval_dev_objects.items():\n",
    "        spans = value[1:]\n",
    "        ne = value[0]\n",
    "        all_tokens = []\n",
    "        for span in spans:\n",
    "            span_obj = factrueval_dev_spans[span]\n",
    "            token = int(span_obj[3])\n",
    "            num_of_tokens = int(span_obj[4])\n",
    "            for i in range(num_of_tokens):\n",
    "                all_tokens.append(token + i)\n",
    "        all_ne.append([ne, sorted(all_tokens)])\n",
    "\n",
    "    for ne_tokens in all_ne:\n",
    "        ne = ne_tokens[0]\n",
    "        token = ne_tokens[1]\n",
    "        for i in range(len(token)):\n",
    "            if token[i] in factrueval_dev_tokens.keys():\n",
    "                if len(token) == 1:\n",
    "                    factrueval_dev_tokens[token[i]].append(\"S-\" + ne)\n",
    "                elif (i == 0 and token[i + 1] - token[i] > 1) or (i == len(token) - 1 and token[i] - token[i - 1] > 1) or (token[i] - token[i - 1] > 1 and token[i + 1] - token[i] > 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"S-\" + ne)\n",
    "                elif (i == 0  and token[i + 1] - token[i] == 1) or (i != len(token) - 1 and token[i] - token[i - 1] > 1 and token[i + 1] - token[i] == 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"B-\" + ne)\n",
    "                elif (i == len(token) - 1 and token[i] - token[i - 1] == 1) or (i != 0 and token[i] - token[i - 1] == 1 and token[i + 1] - token[i] > 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"E-\" + ne)\n",
    "                else: \n",
    "                    factrueval_dev_tokens[token[i]].append(\"I-\" + ne)\n",
    "\n",
    "    for i in range(len(factrueval_dev_tokens_list)):\n",
    "        if factrueval_dev_tokens_list[i] == []:\n",
    "            continue\n",
    "        number_of_token = factrueval_dev_tokens_list[i][0]\n",
    "        if int(number_of_token) in factrueval_dev_tokens.keys() and len(factrueval_dev_tokens[int(number_of_token)]) >= 4:\n",
    "            ne = factrueval_dev_tokens[int(number_of_token)][3]\n",
    "            factrueval_dev_tokens_list[i].append(ne)\n",
    "        else:\n",
    "            factrueval_dev_tokens_list[i].append(\"O\")\n",
    "\n",
    "    final = []\n",
    "    for el in factrueval_dev_tokens_list:\n",
    "        if el == []:\n",
    "            final.append(el)\n",
    "        else:\n",
    "            final.append([el[3], el[1], el[2], el[4]])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = prepare('devset')\n",
    "with open('./devset.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(\"-DOCSTART- O\\n\")\n",
    "    for line in devset:\n",
    "        if line == []:\n",
    "            file.write(\"\\n\")\n",
    "        else:\n",
    "            file.write(\"{} {} {} {}\\n\".format(*line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = prepare('testset')\n",
    "with open('./testset.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(\"-DOCSTART- O\\n\")\n",
    "    for line in testset:\n",
    "        if line == []:\n",
    "            file.write(\"\\n\")\n",
    "        else:\n",
    "            file.write(\"{} {} {} {}\\n\".format(*line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['В', '0', '1', 'O'],\n",
       " ['понедельник', '2', '11', 'O'],\n",
       " ['28', '14', '2', 'O'],\n",
       " ['июня', '17', '4', 'O'],\n",
       " ['у', '22', '1', 'O'],\n",
       " ['здания', '24', '6', 'O'],\n",
       " ['мэрии', '31', '5', 'B-Org'],\n",
       " ['Москвы', '37', '6', 'E-Org'],\n",
       " ['на', '44', '2', 'O'],\n",
       " ['Тверской', '47', '8', 'B-Location'],\n",
       " ['площади', '56', '7', 'E-Location'],\n",
       " ['состоялась', '64', '10', 'O'],\n",
       " ['очередная', '75', '9', 'O'],\n",
       " ['несанкционированная', '85', '19', 'O'],\n",
       " ['акция', '105', '5', 'O'],\n",
       " ['протеста', '111', '8', 'O'],\n",
       " ['«', '120', '1', 'O'],\n",
       " ['День', '121', '4', 'O'],\n",
       " ['гнева', '126', '5', 'O'],\n",
       " ['»', '131', '1', 'O'],\n",
       " [',', '132', '1', 'O'],\n",
       " ['в', '134', '1', 'O'],\n",
       " ['этот', '136', '4', 'O'],\n",
       " ['раз', '141', '3', 'O'],\n",
       " ['направленная', '145', '12', 'O'],\n",
       " [',', '157', '1', 'O'],\n",
       " ['главным', '159', '7', 'O'],\n",
       " ['образом', '167', '7', 'O'],\n",
       " [',', '174', '1', 'O'],\n",
       " ['против', '176', '6', 'O'],\n",
       " ['политики', '183', '8', 'O'],\n",
       " ['московских', '192', '10', 'O'],\n",
       " ['и', '203', '1', 'O'],\n",
       " ['подмосковных', '205', '12', 'O'],\n",
       " ['властей', '218', '7', 'O'],\n",
       " ['.', '225', '1', 'O'],\n",
       " [],\n",
       " ['Среди', '227', '5', 'O'],\n",
       " ['требований', '233', '10', 'O'],\n",
       " [',', '243', '1', 'O'],\n",
       " ['выдвигаемых', '245', '11', 'O'],\n",
       " ['организаторами', '257', '14', 'O'],\n",
       " ['акции', '272', '5', 'O'],\n",
       " [':', '277', '1', 'O'],\n",
       " ['«', '279', '1', 'O'],\n",
       " ['немедленная', '280', '11', 'O'],\n",
       " ['отставка', '292', '8', 'O'],\n",
       " ['мэра', '301', '4', 'O'],\n",
       " ['Москвы', '306', '6', 'S-LocOrg'],\n",
       " ['Юрия', '313', '4', 'B-Person'],\n",
       " ['Лужкова', '318', '7', 'E-Person'],\n",
       " [',', '325', '1', 'O'],\n",
       " ['расследование', '327', '13', 'O'],\n",
       " ['итогов', '341', '6', 'O'],\n",
       " ['его', '348', '3', 'O'],\n",
       " ['деятельности', '352', '12', 'O'],\n",
       " ['»', '364', '1', 'O'],\n",
       " [',', '365', '1', 'O'],\n",
       " ['«', '367', '1', 'O'],\n",
       " ['созыв', '368', '5', 'O'],\n",
       " ['московского', '374', '11', 'O'],\n",
       " ['общественного', '386', '13', 'O'],\n",
       " ['форума', '400', '6', 'O'],\n",
       " ['для', '407', '3', 'O'],\n",
       " ['обсуждения', '411', '10', 'O'],\n",
       " ['путей', '422', '5', 'O'],\n",
       " ['реформирования', '428', '14', 'O'],\n",
       " ['основных', '443', '8', 'O'],\n",
       " ['сфер', '452', '4', 'O'],\n",
       " ['жизнедеятельности', '457', '17', 'O'],\n",
       " ['в', '475', '1', 'O'],\n",
       " ['Москве', '477', '6', 'S-Location'],\n",
       " ['»', '483', '1', 'O'],\n",
       " [',', '484', '1', 'O'],\n",
       " ['«', '486', '1', 'O'],\n",
       " ['восстановление', '487', '14', 'O'],\n",
       " ['прямых', '502', '6', 'O'],\n",
       " ['выборов', '509', '7', 'O'],\n",
       " ['глав', '517', '4', 'O'],\n",
       " ['регионов', '522', '8', 'O'],\n",
       " ['России', '531', '6', 'S-LocOrg'],\n",
       " ['»', '537', '1', 'O'],\n",
       " [',', '538', '1', 'O'],\n",
       " ['«', '540', '1', 'O'],\n",
       " ['роспуск', '541', '7', 'O'],\n",
       " ['нелегитимной', '549', '12', 'O'],\n",
       " ['Мосгордумы', '562', '10', 'S-Org'],\n",
       " ['»', '572', '1', 'O'],\n",
       " [',', '573', '1', 'O'],\n",
       " ['отставка', '575', '8', 'O'],\n",
       " ['подмосковного', '584', '13', 'S-LocOrg'],\n",
       " ['губернатора', '598', '11', 'O'],\n",
       " ['Бориса', '610', '6', 'B-Person'],\n",
       " ['Громова', '617', '7', 'E-Person'],\n",
       " ['и', '625', '1', 'O'],\n",
       " ['др', '627', '2', 'O'],\n",
       " ['.', '629', '1', 'O'],\n",
       " [],\n",
       " ['Участникам', '631', '10', 'O'],\n",
       " ['акции', '642', '5', 'O'],\n",
       " ['предлагалось', '648', '12', 'O'],\n",
       " ['принести', '661', '8', 'O'],\n",
       " ['с', '670', '1', 'O'],\n",
       " ['собой', '672', '5', 'O'],\n",
       " ['лист', '678', '4', 'O'],\n",
       " ['бумаги', '683', '6', 'O'],\n",
       " ['или', '690', '3', 'O'],\n",
       " ['кусок', '694', '5', 'O'],\n",
       " ['ткани', '700', '5', 'O'],\n",
       " ['чёрного', '706', '7', 'O'],\n",
       " ['цвета', '714', '5', 'O'],\n",
       " [',', '719', '1', 'O'],\n",
       " ['символизирующие', '721', '15', 'O'],\n",
       " ['«', '737', '1', 'O'],\n",
       " ['чёрную', '738', '6', 'O'],\n",
       " ['метку', '745', '5', 'O'],\n",
       " ['»', '750', '1', 'O'],\n",
       " ['для', '752', '3', 'O'],\n",
       " ['Юрия', '756', '4', 'B-Person'],\n",
       " ['Лужкова', '761', '7', 'E-Person'],\n",
       " ['.', '768', '1', 'O'],\n",
       " [],\n",
       " ['Начало', '771', '6', 'O'],\n",
       " ['акции', '778', '5', 'O'],\n",
       " ['было', '784', '4', 'O'],\n",
       " ['намечено', '789', '8', 'O'],\n",
       " ['на', '798', '2', 'O'],\n",
       " ['19', '801', '2', 'O'],\n",
       " ['часов', '804', '5', 'O'],\n",
       " [';', '809', '1', 'O'],\n",
       " ['подчёркивалось', '811', '14', 'O'],\n",
       " [',', '825', '1', 'O'],\n",
       " ['что', '827', '3', 'O'],\n",
       " ['она', '831', '3', 'O'],\n",
       " ['состоится', '835', '9', 'O'],\n",
       " ['несмотря', '845', '8', 'O'],\n",
       " ['на', '854', '2', 'O'],\n",
       " ['запрет', '857', '6', 'O'],\n",
       " ['властей', '864', '7', 'O'],\n",
       " ['.', '871', '1', 'O'],\n",
       " [],\n",
       " ['Освещающие', '873', '10', 'O'],\n",
       " ['акцию', '884', '5', 'O'],\n",
       " ['блоггеры', '890', '8', 'O'],\n",
       " ['сообщили', '899', '8', 'O'],\n",
       " [',', '907', '1', 'O'],\n",
       " ['что', '909', '3', 'O'],\n",
       " ['автобусы', '913', '8', 'O'],\n",
       " ['с', '922', '1', 'O'],\n",
       " ['милицией', '924', '8', 'O'],\n",
       " ['стали', '933', '5', 'O'],\n",
       " ['занимать', '939', '8', 'O'],\n",
       " ['площадь', '948', '7', 'O'],\n",
       " ['у', '956', '1', 'O'],\n",
       " ['памятника', '958', '9', 'B-Location'],\n",
       " ['Юрию', '968', '4', 'I-Location'],\n",
       " ['Долгорукому', '973', '11', 'E-Location'],\n",
       " ['ещё', '985', '3', 'O'],\n",
       " ['с', '989', '1', 'O'],\n",
       " ['15', '991', '2', 'O'],\n",
       " ['часов', '994', '5', 'O'],\n",
       " ['дня', '1000', '3', 'O'],\n",
       " [',', '1003', '1', 'O'],\n",
       " ['центральная', '1005', '11', 'O'],\n",
       " ['часть', '1017', '5', 'O'],\n",
       " ['площади', '1023', '7', 'O'],\n",
       " ['была', '1031', '4', 'O'],\n",
       " ['огорожена', '1036', '9', 'O'],\n",
       " ['.', '1045', '1', 'O'],\n",
       " [],\n",
       " ['Ко', '1047', '2', 'O'],\n",
       " ['времени', '1050', '7', 'O'],\n",
       " ['начала', '1058', '6', 'O'],\n",
       " ['акции', '1065', '5', 'O'],\n",
       " ['вокруг', '1071', '6', 'O'],\n",
       " ['огороженной', '1078', '11', 'O'],\n",
       " ['территории', '1090', '10', 'O'],\n",
       " ['собралось', '1101', '9', 'O'],\n",
       " ['множество', '1111', '9', 'O'],\n",
       " ['журналистов', '1121', '11', 'O'],\n",
       " ['и', '1133', '1', 'O'],\n",
       " ['прохожих', '1135', '8', 'O'],\n",
       " [',', '1143', '1', 'O'],\n",
       " ['по', '1145', '2', 'O'],\n",
       " ['мере', '1148', '4', 'O'],\n",
       " ['прибытия', '1153', '8', 'O'],\n",
       " ['самих', '1162', '5', 'O'],\n",
       " ['участников', '1168', '10', 'O'],\n",
       " ['акции', '1179', '5', 'O'],\n",
       " ['милиция', '1185', '7', 'O'],\n",
       " ['начала', '1193', '6', 'O'],\n",
       " ['планомерно', '1200', '10', 'O'],\n",
       " ['их', '1211', '2', 'O'],\n",
       " ['задерживать', '1214', '11', 'O'],\n",
       " ['и', '1226', '1', 'O'],\n",
       " ['заталкивать', '1228', '11', 'O'],\n",
       " ['в', '1240', '1', 'O'],\n",
       " ['автобусы', '1242', '8', 'O'],\n",
       " ['.', '1250', '1', 'O'],\n",
       " [],\n",
       " ['Всего', '1253', '5', 'O'],\n",
       " [',', '1258', '1', 'O'],\n",
       " ['по', '1260', '2', 'O'],\n",
       " ['сообщениям', '1263', '10', 'O'],\n",
       " ['блоггеров', '1274', '9', 'O'],\n",
       " ['и', '1284', '1', 'O'],\n",
       " ['СМИ', '1286', '3', 'O'],\n",
       " [',', '1289', '1', 'O'],\n",
       " ['было', '1291', '4', 'O'],\n",
       " ['задержано', '1296', '9', 'O'],\n",
       " ['более', '1306', '5', 'O'],\n",
       " ['30', '1312', '2', 'O'],\n",
       " ['человек', '1315', '7', 'O'],\n",
       " ['.', '1322', '1', 'O'],\n",
       " [],\n",
       " ['Пользователь', '1324', '12', 'O'],\n",
       " ['ЖЖ', '1337', '2', 'O'],\n",
       " ['zyalt', '1340', '5', 'S-Person'],\n",
       " ['сообщает', '1346', '8', 'O'],\n",
       " [',', '1354', '1', 'O'],\n",
       " ['что', '1356', '3', 'O'],\n",
       " ['среди', '1360', '5', 'O'],\n",
       " ['задержанных', '1366', '11', 'O'],\n",
       " ['оказался', '1378', '8', 'O'],\n",
       " ['депутат', '1387', '7', 'O'],\n",
       " ['муниципального', '1395', '14', 'B-Org'],\n",
       " ['собрания', '1410', '8', 'I-Org'],\n",
       " ['района', '1419', '6', 'I-Org'],\n",
       " ['Отрадное', '1426', '8', 'E-Org'],\n",
       " [',', '1434', '1', 'O'],\n",
       " ['сопредседатель', '1436', '14', 'O'],\n",
       " ['московского', '1451', '11', 'B-Org'],\n",
       " ['отделения', '1463', '9', 'I-Org'],\n",
       " ['движения', '1473', '8', 'S-Org'],\n",
       " ['«', '1482', '1', 'O'],\n",
       " ['Солидарность', '1483', '12', 'S-Org'],\n",
       " ['»', '1495', '1', 'O'],\n",
       " ['Михаил', '1497', '6', 'B-Person'],\n",
       " ['Вельмакин', '1504', '9', 'E-Person'],\n",
       " [',', '1513', '1', 'O'],\n",
       " ['известный', '1515', '9', 'O'],\n",
       " ['правозащитник', '1525', '13', 'O'],\n",
       " ['Лев', '1539', '3', 'B-Person'],\n",
       " ['Пономарев', '1543', '9', 'E-Person'],\n",
       " [',', '1552', '1', 'O'],\n",
       " ['координатор', '1554', '11', 'O'],\n",
       " ['движения', '1566', '8', 'S-Org'],\n",
       " ['«', '1575', '1', 'O'],\n",
       " ['Левый', '1576', '5', 'B-Org'],\n",
       " ['фронт', '1582', '5', 'E-Org'],\n",
       " ['»', '1587', '1', 'O'],\n",
       " ['Сергей', '1589', '6', 'B-Person'],\n",
       " ['Удальцов', '1596', '8', 'E-Person'],\n",
       " ['.', '1604', '1', 'O'],\n",
       " [],\n",
       " ['Организаторами', '1607', '14', 'O'],\n",
       " ['акции', '1622', '5', 'O'],\n",
       " ['выступили', '1628', '9', 'O'],\n",
       " ['движение', '1638', '8', 'S-Org'],\n",
       " ['«', '1647', '1', 'O'],\n",
       " ['За', '1648', '2', 'B-Org'],\n",
       " ['права', '1651', '5', 'I-Org'],\n",
       " ['человека', '1657', '8', 'E-Org'],\n",
       " ['»', '1665', '1', 'O'],\n",
       " [',', '1666', '1', 'O'],\n",
       " ['Союз', '1668', '4', 'I-Org'],\n",
       " ['координационных', '1673', '15', 'I-Org'],\n",
       " ['советов', '1689', '7', 'E-Org'],\n",
       " ['(', '1697', '1', 'O'],\n",
       " ['СКС', '1698', '3', 'S-Org'],\n",
       " [')', '1701', '1', 'O'],\n",
       " [',', '1702', '1', 'O'],\n",
       " ['институт', '1704', '8', 'S-Org'],\n",
       " ['«', '1713', '1', 'O'],\n",
       " ['Коллективное', '1714', '12', 'B-Org'],\n",
       " ['действие', '1727', '8', 'E-Org'],\n",
       " ['»', '1735', '1', 'O'],\n",
       " [',', '1736', '1', 'O'],\n",
       " ['«', '1738', '1', 'O'],\n",
       " ['Жилищная', '1739', '8', 'B-Org'],\n",
       " ['солидарность', '1748', '12', 'E-Org'],\n",
       " ['»', '1760', '1', 'O'],\n",
       " [',', '1761', '1', 'O'],\n",
       " ['Движение', '1763', '8', 'I-Org'],\n",
       " ['общежитий', '1772', '9', 'I-Org'],\n",
       " ['Москвы', '1782', '6', 'E-Org'],\n",
       " [',', '1788', '1', 'O'],\n",
       " ['Движение', '1790', '8', 'I-Org'],\n",
       " ['в', '1799', '1', 'I-Org'],\n",
       " ['защиту', '1801', '6', 'I-Org'],\n",
       " ['Химкинского', '1808', '11', 'I-Org'],\n",
       " ['леса', '1820', '4', 'E-Org'],\n",
       " [',', '1824', '1', 'O'],\n",
       " ['движение', '1826', '8', 'S-Org'],\n",
       " ['«', '1835', '1', 'O'],\n",
       " ['Московский', '1836', '10', 'B-Org'],\n",
       " ['совет', '1847', '5', 'E-Org'],\n",
       " ['»', '1852', '1', 'O'],\n",
       " [',', '1853', '1', 'O'],\n",
       " ['«', '1855', '1', 'O'],\n",
       " ['Координационный', '1856', '15', 'I-Org'],\n",
       " ['совет', '1872', '5', 'I-Org'],\n",
       " ['пострадавших', '1878', '12', 'I-Org'],\n",
       " ['соинвесторов', '1891', '12', 'E-Org'],\n",
       " ['»', '1903', '1', 'O'],\n",
       " ['.', '1904', '1', 'O'],\n",
       " [],\n",
       " ['Конец', '0', '5', 'O'],\n",
       " ['света', '6', '5', 'O'],\n",
       " [',', '11', '1', 'O'],\n",
       " ['который', '13', '7', 'O'],\n",
       " ['предрекал', '21', '9', 'O'],\n",
       " ['проповедник', '31', '11', 'O'],\n",
       " ['-', '42', '1', 'O'],\n",
       " ['евангелист', '43', '10', 'O'],\n",
       " ['Герольд', '54', '7', 'B-Person'],\n",
       " ['Кемпинг', '62', '7', 'E-Person'],\n",
       " ['утром', '70', '5', 'O'],\n",
       " ['в', '76', '1', 'O'],\n",
       " ['06.00', '78', '5', 'O'],\n",
       " ['по', '84', '2', 'O'],\n",
       " ['московскому', '87', '11', 'O'],\n",
       " ['времени', '99', '7', 'O'],\n",
       " ['21', '107', '2', 'O'],\n",
       " ['мая', '110', '3', 'O'],\n",
       " ['2011', '114', '4', 'O'],\n",
       " ['года', '119', '4', 'O'],\n",
       " ['так', '124', '3', 'O'],\n",
       " ['и', '128', '1', 'O'],\n",
       " ['не', '130', '2', 'O'],\n",
       " ['наступил', '133', '8', 'O'],\n",
       " ['.', '141', '1', 'O'],\n",
       " [],\n",
       " ['Это', '144', '3', 'O'],\n",
       " ['далеко', '148', '6', 'O'],\n",
       " ['не', '155', '2', 'O'],\n",
       " ['первое', '158', '6', 'O'],\n",
       " ['несбывшееся', '165', '11', 'O'],\n",
       " ['предсказание', '177', '12', 'O'],\n",
       " ['апокалипсиса', '190', '12', 'O'],\n",
       " ['.', '202', '1', 'O'],\n",
       " [],\n",
       " ['Поначалу', '204', '8', 'O'],\n",
       " ['на', '213', '2', 'O'],\n",
       " ['него', '216', '4', 'O'],\n",
       " ['практически', '221', '11', 'O'],\n",
       " ['не', '233', '2', 'O'],\n",
       " ['обратили', '236', '8', 'O'],\n",
       " ['внимания', '245', '8', 'O'],\n",
       " [',', '253', '1', 'O'],\n",
       " ['однако', '255', '6', 'O'],\n",
       " [',', '261', '1', 'O'],\n",
       " ['с', '263', '1', 'O'],\n",
       " ['приближением', '265', '12', 'O'],\n",
       " ['этой', '278', '4', 'O'],\n",
       " ['даты', '283', '4', 'O'],\n",
       " [',', '287', '1', 'O'],\n",
       " ['немало', '289', '6', 'O'],\n",
       " ['СМИ', '296', '3', 'O'],\n",
       " ['в', '300', '1', 'O'],\n",
       " ['поисках', '302', '7', 'O'],\n",
       " ['сюжета', '310', '6', 'O'],\n",
       " [',', '316', '1', 'O'],\n",
       " ['останавливались', '318', '15', 'O'],\n",
       " ['на', '334', '2', 'O'],\n",
       " ['этом', '337', '4', 'O'],\n",
       " [',', '341', '1', 'O'],\n",
       " ['создавая', '343', '8', 'O'],\n",
       " ['прекрасную', '352', '10', 'O'],\n",
       " ['бесплатную', '363', '10', 'O'],\n",
       " ['PR-компанию', '374', '11', 'O'],\n",
       " ['для', '386', '3', 'O'],\n",
       " ['горе-пророка', '390', '12', 'O'],\n",
       " ['.', '402', '1', 'O'],\n",
       " [],\n",
       " ['Вполне', '404', '6', 'O'],\n",
       " ['вероятно', '411', '8', 'O'],\n",
       " [',', '419', '1', 'O'],\n",
       " ['что', '421', '3', 'O'],\n",
       " ['Герольд', '425', '7', 'B-Person'],\n",
       " ['Кемпинг', '433', '7', 'E-Person'],\n",
       " ['объяснит', '441', '8', 'O'],\n",
       " ['это', '450', '3', 'O'],\n",
       " ['какой-либо', '454', '10', 'O'],\n",
       " ['неточностью', '465', '11', 'O'],\n",
       " ['и', '477', '1', 'O'],\n",
       " ['перенесет', '479', '9', 'O'],\n",
       " ['катастрофу', '489', '10', 'O'],\n",
       " ['на', '500', '2', 'O'],\n",
       " ['некоторое', '503', '9', 'O'],\n",
       " ['время', '513', '5', 'O'],\n",
       " ['вперёд', '519', '6', 'O'],\n",
       " [',', '525', '1', 'O'],\n",
       " ['и', '527', '1', 'O'],\n",
       " ['по', '529', '2', 'O'],\n",
       " ['прежнему', '532', '8', 'O'],\n",
       " ['будет', '541', '5', 'O'],\n",
       " ['оставаться', '547', '10', 'O'],\n",
       " ['в', '558', '1', 'O'],\n",
       " ['центре', '560', '6', 'O'],\n",
       " ['внимания', '567', '8', 'O'],\n",
       " ['.', '575', '1', 'O'],\n",
       " [],\n",
       " ['Так', '577', '3', 'O'],\n",
       " [',', '580', '1', 'O'],\n",
       " ['согласно', '582', '8', 'O'],\n",
       " ['его', '591', '3', 'O'],\n",
       " ['пророчествам', '595', '12', 'O'],\n",
       " [',', '607', '1', 'O'],\n",
       " ['6', '609', '1', 'O'],\n",
       " ['сентября', '611', '8', 'O'],\n",
       " ['1994', '620', '4', 'O'],\n",
       " ['года', '625', '4', 'O'],\n",
       " ['на', '630', '2', 'O'],\n",
       " ['Землю', '633', '5', 'S-Location'],\n",
       " ['должен', '639', '6', 'O'],\n",
       " ['был', '646', '3', 'O'],\n",
       " ['опуститься', '650', '10', 'O'],\n",
       " ['Иисус', '661', '5', 'B-Person'],\n",
       " ['Христос', '667', '7', 'E-Person'],\n",
       " ['.', '674', '1', 'O'],\n",
       " [],\n",
       " ['Следующее', '677', '9', 'O'],\n",
       " ['подобное', '687', '8', 'O'],\n",
       " ['мероприятие', '696', '11', 'O'],\n",
       " [',', '707', '1', 'O'],\n",
       " ['по', '709', '2', 'O'],\n",
       " ['прогнозам', '712', '9', 'O'],\n",
       " ['майя', '722', '4', 'O'],\n",
       " [',', '726', '1', 'O'],\n",
       " ['ожидается', '728', '9', 'O'],\n",
       " ['21', '738', '2', 'O'],\n",
       " ['декабря', '741', '7', 'O'],\n",
       " ['2012', '749', '4', 'O'],\n",
       " ['года', '754', '4', 'O'],\n",
       " [',', '758', '1', 'O'],\n",
       " ['однако', '760', '6', 'O'],\n",
       " ['не', '767', '2', 'O'],\n",
       " ['следует', '770', '7', 'O'],\n",
       " ['забывать', '778', '8', 'O'],\n",
       " [',', '786', '1', 'O'],\n",
       " ['что', '788', '3', 'O'],\n",
       " ['конец', '792', '5', 'O'],\n",
       " ['этой', '798', '4', 'O'],\n",
       " ['цивилизации', '803', '11', 'O'],\n",
       " ['наступил', '815', '8', 'O'],\n",
       " ['значительно', '824', '11', 'O'],\n",
       " ['раньше', '836', '6', 'O'],\n",
       " [',', '842', '1', 'O'],\n",
       " ['и', '844', '1', 'O'],\n",
       " ['это', '846', '3', 'O'],\n",
       " ['отчего-то', '850', '9', 'O'],\n",
       " ['не', '860', '2', 'O'],\n",
       " ['отображено', '863', '10', 'O'],\n",
       " ['в', '874', '1', 'O'],\n",
       " ['их', '876', '2', 'O'],\n",
       " ['наследии', '879', '8', 'O'],\n",
       " [',', '887', '1', 'O'],\n",
       " ['поэтому', '889', '7', 'O'],\n",
       " ['и', '897', '1', 'O'],\n",
       " ['достоверность', '899', '13', 'O'],\n",
       " ['подобного', '913', '9', 'O'],\n",
       " ['прорицания', '923', '10', 'O'],\n",
       " ['далеко', '934', '6', 'O'],\n",
       " ['не', '941', '2', 'O'],\n",
       " ['очевидна', '944', '8', 'O'],\n",
       " ['.', '952', '1', 'O'],\n",
       " [],\n",
       " ['Если', '955', '4', 'O'],\n",
       " ['-', '959', '1', 'O'],\n",
       " ['же', '960', '2', 'O'],\n",
       " ['и', '963', '1', 'O'],\n",
       " ['майя', '965', '4', 'O'],\n",
       " ['ошиблись', '970', '8', 'O'],\n",
       " [',', '978', '1', 'O'],\n",
       " ['то', '980', '2', 'O'],\n",
       " ['расслабляться', '983', '13', 'O'],\n",
       " ['всё-равно', '997', '9', 'O'],\n",
       " ['не', '1007', '2', 'O'],\n",
       " ['стоит', '1010', '5', 'O'],\n",
       " ['.', '1015', '1', 'O'],\n",
       " [],\n",
       " ['Апокалиптических', '1017', '16', 'O'],\n",
       " ['теорий', '1034', '6', 'O'],\n",
       " ['ещё', '1041', '3', 'O'],\n",
       " ['достаточно', '1045', '10', 'O'],\n",
       " [':', '1055', '1', 'O'],\n",
       " [],\n",
       " ['*', '1058', '1', 'O'],\n",
       " ['2021', '1060', '4', 'O'],\n",
       " ['год', '1065', '3', 'O'],\n",
       " ['—', '1069', '1', 'O'],\n",
       " ['инверсия', '1071', '8', 'O'],\n",
       " ['магнитного', '1080', '10', 'O'],\n",
       " ['поля', '1091', '4', 'O'],\n",
       " ['Земли', '1096', '5', 'S-Location'],\n",
       " ['уничтожит', '1102', '9', 'O'],\n",
       " ['часть', '1112', '5', 'O'],\n",
       " ['человечества', '1118', '12', 'O'],\n",
       " ['.', '1130', '1', 'O'],\n",
       " [],\n",
       " ['*', '1132', '1', 'O'],\n",
       " ['2060', '1134', '4', 'O'],\n",
       " ['год', '1139', '3', 'O'],\n",
       " ['—', '1143', '1', 'O'],\n",
       " ['Согласно', '1145', '8', 'O'],\n",
       " ['расчётам', '1154', '8', 'O'],\n",
       " ['Исаака', '1163', '6', 'B-Person'],\n",
       " ['Ньютона', '1170', '7', 'E-Person'],\n",
       " [',', '1177', '1', 'O'],\n",
       " ['сделанным', '1179', '9', 'O'],\n",
       " ['в', '1189', '1', 'O'],\n",
       " ['1704', '1191', '4', 'O'],\n",
       " ['году', '1196', '4', 'O'],\n",
       " ['по', '1201', '2', 'O'],\n",
       " ['библейской', '1204', '10', 'O'],\n",
       " ['книге', '1215', '5', 'O'],\n",
       " ['Даниила', '1221', '7', 'S-Person'],\n",
       " ['станет', '1229', '6', 'O'],\n",
       " ['последним', '1236', '9', 'O'],\n",
       " ['для', '1246', '3', 'O'],\n",
       " ['людей', '1250', '5', 'O'],\n",
       " ['.', '1255', '1', 'O'],\n",
       " [],\n",
       " ['*', '1257', '1', 'O'],\n",
       " ['2242', '1259', '4', 'O'],\n",
       " ['год', '1264', '3', 'O'],\n",
       " ['—', '1268', '1', 'O'],\n",
       " ['конец', '1270', '5', 'O'],\n",
       " ['эпохи', '1276', '5', 'O'],\n",
       " ['Солнца', '1282', '6', 'S-Location'],\n",
       " ['согласно', '1289', '8', 'O'],\n",
       " ['теории', '1298', '6', 'O'],\n",
       " ['смены', '1305', '5', 'O'],\n",
       " ['планетных', '1311', '9', 'O'],\n",
       " ['эпох', '1321', '4', 'O'],\n",
       " [',', '1325', '1', 'O'],\n",
       " ['которая', '1327', '7', 'O'],\n",
       " ['была', '1335', '4', 'O'],\n",
       " ['изложена', '1340', '8', 'O'],\n",
       " ['в', '1349', '1', 'O'],\n",
       " ['работах', '1351', '7', 'O'],\n",
       " ['ибн', '1359', '3', 'B-Person'],\n",
       " ['Эзра', '1363', '4', 'E-Person'],\n",
       " [',', '1367', '1', 'O'],\n",
       " ['Авраама', '1369', '7', 'S-Person'],\n",
       " ['и', '1377', '1', 'O'],\n",
       " ['Абу', '1379', '3', 'B-Person'],\n",
       " ['Машара', '1383', '6', 'E-Person'],\n",
       " ['.', '1389', '1', 'O'],\n",
       " [],\n",
       " ['*', '1391', '1', 'O'],\n",
       " ['2892', '1393', '4', 'O'],\n",
       " ['год', '1398', '3', 'O'],\n",
       " ['—', '1402', '1', 'O'],\n",
       " ['апокалипсис', '1404', '11', 'O'],\n",
       " ['обещанный', '1416', '9', 'O'],\n",
       " ['Авелем', '1426', '6', 'S-Person'],\n",
       " ['.', '1432', '1', 'O'],\n",
       " [],\n",
       " ['*', '1434', '1', 'O'],\n",
       " ['3797', '1436', '4', 'O'],\n",
       " ['год', '1441', '3', 'O'],\n",
       " ['—', '1445', '1', 'O'],\n",
       " ['кончину', '1447', '7', 'O'],\n",
       " ['миру', '1455', '4', 'O'],\n",
       " ['предрёк', '1460', '7', 'O'],\n",
       " ['сам', '1468', '3', 'O'],\n",
       " ['Нострадамус', '1472', '11', 'S-Person'],\n",
       " ['.', '1483', '1', 'O'],\n",
       " [],\n",
       " ['*', '1485', '1', 'O'],\n",
       " ['5079', '1487', '4', 'O'],\n",
       " ['год', '1492', '3', 'O'],\n",
       " ['—', '1496', '1', 'O'],\n",
       " ['сочла', '1498', '5', 'O'],\n",
       " ['последним', '1504', '9', 'O'],\n",
       " ['всемирно', '1514', '8', 'O'],\n",
       " ['известная', '1523', '9', 'O'],\n",
       " ['Ванга', '1533', '5', 'S-Person'],\n",
       " ['…', '1539', '1', 'O'],\n",
       " [],\n",
       " ['Промышленный', '0', '12', 'B-Org'],\n",
       " ['и', '13', '1', 'I-Org'],\n",
       " ['коммерческий', '15', '12', 'I-Org'],\n",
       " ['банк', '28', '4', 'I-Org'],\n",
       " ['Китая', '33', '5', 'E-Org'],\n",
       " ['(', '39', '1', 'O'],\n",
       " ['Industrial', '40', '10', 'B-Org'],\n",
       " ['and', '51', '3', 'I-Org'],\n",
       " ['Commercial', '55', '10', 'I-Org'],\n",
       " ['Bank', '66', '4', 'I-Org'],\n",
       " ['of', '71', '2', 'I-Org'],\n",
       " ['China', '74', '5', 'E-Org'],\n",
       " [')', '79', '1', 'O'],\n",
       " [',', '80', '1', 'O'],\n",
       " ['по', '82', '2', 'O'],\n",
       " ['некоторым', '85', '9', 'O'],\n",
       " ['показателям', '95', '11', 'O'],\n",
       " ['—', '107', '1', 'O'],\n",
       " ['крупнейший', '109', '10', 'O'],\n",
       " ['банк', '120', '4', 'O'],\n",
       " ['мира', '125', '4', 'O'],\n",
       " [',', '129', '1', 'O'],\n",
       " ['подписал', '131', '8', 'O'],\n",
       " ['соглашение', '140', '10', 'O'],\n",
       " ['о', '151', '1', 'O'],\n",
       " ['покупке', '153', '7', 'O'],\n",
       " ['контрольной', '161', '11', 'O'],\n",
       " ['доли', '173', '4', 'O'],\n",
       " ['в', '178', '1', 'O'],\n",
       " ['американском', '180', '12', 'B-Org'],\n",
       " ['отделении', '193', '9', 'I-Org'],\n",
       " ['Bank', '203', '4', 'I-Org'],\n",
       " ['of', '208', '2', 'I-Org'],\n",
       " ['East', '211', '4', 'I-Org'],\n",
       " ['Asia', '216', '4', 'E-Org'],\n",
       " [',', '220', '1', 'O'],\n",
       " ['управляющим', '222', '11', 'O'],\n",
       " ['13-ю', '234', '4', 'O'],\n",
       " ['филиалами', '239', '9', 'O'],\n",
       " ['в', '249', '1', 'O'],\n",
       " ['Калифорнии', '251', '10', 'S-Location'],\n",
       " ['и', '262', '1', 'O'],\n",
       " ['Нью-Йорке', '264', '9', 'S-Location'],\n",
       " ['.', '273', '1', 'O'],\n",
       " [],\n",
       " ['За', '275', '2', 'O'],\n",
       " ['80', '278', '2', 'O'],\n",
       " ['процентов', '281', '9', 'O'],\n",
       " ['акций', '291', '5', 'O'],\n",
       " ['этой', '297', '4', 'O'],\n",
       " ['финансовой', '302', '10', 'O'],\n",
       " ['структуры', '313', '9', 'O'],\n",
       " [',', '322', '1', 'O'],\n",
       " ['подконтрольный', '324', '14', 'B-Org'],\n",
       " ['китайскому', '339', '10', 'B-Org'],\n",
       " ['правительству', '350', '13', 'E-Org'],\n",
       " ['банк', '364', '4', 'I-Org'],\n",
       " ['заплатит', '369', '8', 'O'],\n",
       " ['около', '378', '5', 'O'],\n",
       " ['100', '384', '3', 'O'],\n",
       " ['млн', '388', '3', 'O'],\n",
       " ['долларов', '392', '8', 'O'],\n",
       " ['.', '400', '1', 'O'],\n",
       " [],\n",
       " ['Соглашение', '403', '10', 'O'],\n",
       " ['было', '414', '4', 'O'],\n",
       " ['подписано', '419', '9', 'O'],\n",
       " ['в', '429', '1', 'O'],\n",
       " ['минувшую', '431', '8', 'O'],\n",
       " ['пятницу', '440', '7', 'O'],\n",
       " ['в', '448', '1', 'O'],\n",
       " ['Чикаго', '450', '6', 'S-Location'],\n",
       " [',', '456', '1', 'O'],\n",
       " ['в', '458', '1', 'O'],\n",
       " ['последний', '460', '9', 'O'],\n",
       " ['день', '470', '4', 'O'],\n",
       " ['визита', '475', '6', 'O'],\n",
       " ['председателя', '482', '12', 'O'],\n",
       " ['КНР', '495', '3', 'S-LocOrg'],\n",
       " ['Ху', '499', '2', 'B-Person'],\n",
       " ['Цзиньтао', '502', '8', 'E-Person'],\n",
       " ['в', '511', '1', 'O'],\n",
       " ['США', '513', '3', 'S-Location'],\n",
       " ['.', '516', '1', 'O'],\n",
       " [],\n",
       " ['Сделка', '518', '6', 'O'],\n",
       " ['ожидает', '525', '7', 'O'],\n",
       " ['утверждения', '533', '11', 'O'],\n",
       " ['американскими', '545', '13', 'O'],\n",
       " ['регулирующими', '559', '13', 'O'],\n",
       " ['органами', '573', '8', 'O'],\n",
       " [',', '581', '1', 'O'],\n",
       " ['что', '583', '3', 'O'],\n",
       " ['может', '587', '5', 'O'],\n",
       " ['задержать', '593', '9', 'O'],\n",
       " ['ее', '603', '2', 'O'],\n",
       " ['завершение', '606', '10', 'O'],\n",
       " ['до', '617', '2', 'O'],\n",
       " ['конца', '620', '5', 'O'],\n",
       " ['этого', '626', '5', 'O'],\n",
       " ['года', '632', '4', 'O'],\n",
       " ['.', '636', '1', 'O'],\n",
       " [],\n",
       " ['Доля', '639', '4', 'O'],\n",
       " ['правительства', '644', '13', 'B-Org'],\n",
       " ['Китая', '658', '5', 'E-Org'],\n",
       " ['в', '664', '1', 'O'],\n",
       " ['Industrial', '666', '10', 'B-Org'],\n",
       " ['and', '677', '3', 'I-Org'],\n",
       " ['Commercial', '681', '10', 'I-Org'],\n",
       " ['Bank', '692', '4', 'I-Org'],\n",
       " ['of', '697', '2', 'I-Org'],\n",
       " ['China', '700', '5', 'E-Org'],\n",
       " ['составляет', '706', '10', 'O'],\n",
       " ['70', '717', '2', 'O'],\n",
       " ['процентов', '720', '9', 'O'],\n",
       " ['.', '729', '1', 'O'],\n",
       " [],\n",
       " ['Если', '731', '4', 'O'],\n",
       " ['сделка', '736', '6', 'O'],\n",
       " ['будет', '743', '5', 'O'],\n",
       " ['утверждена', '749', '10', 'O'],\n",
       " [',', '759', '1', 'O'],\n",
       " ['то', '761', '2', 'O'],\n",
       " ['государственный', '764', '15', 'B-Org'],\n",
       " ['китайский', '780', '9', 'I-Org'],\n",
       " ['банк', '790', '4', 'I-Org'],\n",
       " ['впервые', '795', '7', 'O'],\n",
       " ['в', '803', '1', 'O'],\n",
       " ['истории', '805', '7', 'O'],\n",
       " ['получит', '813', '7', 'O'],\n",
       " ['контроль', '821', '8', 'O'],\n",
       " ['над', '830', '3', 'O'],\n",
       " ['американским', '834', '12', 'O'],\n",
       " ['финансовым', '847', '10', 'O'],\n",
       " ['учреждением', '858', '11', 'O'],\n",
       " ['.', '869', '1', 'O'],\n",
       " [],\n",
       " ['Американцы', '871', '10', 'O'],\n",
       " ['смогут', '882', '6', 'O'],\n",
       " ['делать', '889', '6', 'O'],\n",
       " ['вклады', '896', '6', 'O'],\n",
       " ['в', '903', '1', 'O'],\n",
       " ['его', '905', '3', 'O'],\n",
       " ['филиалах', '909', '8', 'O'],\n",
       " [',', '917', '1', 'O'],\n",
       " ['а', '919', '1', 'O'],\n",
       " ['инвесторы', '921', '9', 'O'],\n",
       " ['смогут', '931', '6', 'O'],\n",
       " ['открывать', '938', '9', 'O'],\n",
       " ['счета', '948', '5', 'O'],\n",
       " ['и', '954', '1', 'O'],\n",
       " ['играть', '956', '6', 'O'],\n",
       " ['на', '963', '2', 'O'],\n",
       " ['курсе', '966', '5', 'O'],\n",
       " ['юаня', '972', '4', 'O'],\n",
       " ['.', '976', '1', 'O'],\n",
       " [],\n",
       " ['За', '979', '2', 'O'],\n",
       " ['последние', '982', '9', 'O'],\n",
       " ['годы', '992', '4', 'O'],\n",
       " ['размеры', '997', '7', 'O'],\n",
       " ['иностранных', '1005', '11', 'O'],\n",
       " ['инвестиций', '1017', '10', 'O'],\n",
       " ['Китая', '1028', '5', 'S-LocOrg'],\n",
       " ['резко', '1034', '5', 'O'],\n",
       " ['увеличились', '1040', '11', 'O'],\n",
       " [',', '1051', '1', 'O'],\n",
       " ['однако', '1053', '6', 'O'],\n",
       " ['расширение', '1060', '10', 'O'],\n",
       " ['китайских', '1071', '9', 'O'],\n",
       " ['финансовых', '1081', '10', 'O'],\n",
       " ['учреждений', '1092', '10', 'O'],\n",
       " ['проходило', '1103', '9', 'O'],\n",
       " ['гораздо', '1113', '7', 'O'],\n",
       " ['медленнее', '1121', '9', 'O'],\n",
       " ['.', '1130', '1', 'O'],\n",
       " [],\n",
       " ['При', '1132', '3', 'O'],\n",
       " ['подписании', '1136', '10', 'O'],\n",
       " ['нового', '1147', '6', 'O'],\n",
       " ['соглашения', '1154', '10', 'O'],\n",
       " ['министр', '1165', '7', 'O'],\n",
       " ['торговли', '1173', '8', 'O'],\n",
       " ['Китая', '1182', '5', 'S-LocOrg'],\n",
       " ['Чэнь', '1188', '4', 'B-Person'],\n",
       " ['Дэмин', '1193', '5', 'E-Person'],\n",
       " ['заявил', '1199', '6', 'O'],\n",
       " [',', '1205', '1', 'O'],\n",
       " ['что', '1207', '3', 'O'],\n",
       " ['одним', '1211', '5', 'O'],\n",
       " ['из', '1217', '2', 'O'],\n",
       " ['приоритетов', '1220', '11', 'O'],\n",
       " ['является', '1232', '8', 'O'],\n",
       " ['перевод', '1241', '7', 'O'],\n",
       " ['огромных', '1249', '8', 'O'],\n",
       " ['резервов', '1258', '8', 'O'],\n",
       " ['иностранной', '1267', '11', 'O'],\n",
       " ['валюты', '1279', '6', 'O'],\n",
       " ['в', '1286', '1', 'O'],\n",
       " ['зарубежные', '1288', '10', 'O'],\n",
       " ['активы', '1299', '6', 'O'],\n",
       " ['.', '1305', '1', 'O'],\n",
       " [],\n",
       " ['Барак', '0', '5', 'B-Person'],\n",
       " ['Обама', '6', '5', 'E-Person'],\n",
       " ['принимает', '12', '9', 'O'],\n",
       " ['в', '22', '1', 'O'],\n",
       " ['Белом', '24', '5', 'B-Location'],\n",
       " ['доме', '30', '4', 'I-Location'],\n",
       " ['своего', '35', '6', 'O'],\n",
       " ['французского', '42', '12', 'O'],\n",
       " ['коллегу', '55', '7', 'O'],\n",
       " ['Николя', '63', '6', 'B-Person'],\n",
       " ['Саркози', '70', '7', 'E-Person'],\n",
       " ['.', '77', '1', 'O'],\n",
       " [],\n",
       " ['Как', '79', '3', 'O'],\n",
       " ['было', '83', '4', 'O'],\n",
       " ['объявлено', '88', '9', 'O'],\n",
       " [',', '97', '1', 'O'],\n",
       " ['президент', '99', '9', 'O'],\n",
       " ['Франции', '109', '7', 'S-LocOrg'],\n",
       " ['прибыл', '117', '6', 'O'],\n",
       " ['в', '124', '1', 'O'],\n",
       " ['Вашингтон', '126', '9', 'S-Location'],\n",
       " [',', '135', '1', 'O'],\n",
       " ['чтобы', '137', '5', 'O'],\n",
       " ['обсудить', '143', '8', 'O'],\n",
       " ['с', '152', '1', 'O'],\n",
       " ['главой', '154', '6', 'O'],\n",
       " ['администрации', '161', '13', 'B-Org'],\n",
       " ['США', '175', '3', 'E-Org'],\n",
       " ['ряд', '179', '3', 'O'],\n",
       " ['насущных', '183', '8', 'O'],\n",
       " ['проблем', '192', '7', 'O'],\n",
       " [',', '199', '1', 'O'],\n",
       " ['главное', '201', '7', 'O'],\n",
       " ['место', '209', '5', 'O'],\n",
       " ['среди', '215', '5', 'O'],\n",
       " ['которых', '221', '7', 'O'],\n",
       " ['занимает', '229', '8', 'O'],\n",
       " ['состояние', '238', '9', 'O'],\n",
       " ['мировой', '248', '7', 'O'],\n",
       " ['экономики', '256', '9', 'O'],\n",
       " ['и', '266', '1', 'O'],\n",
       " ['безопасность', '268', '12', 'O'],\n",
       " ['.', '280', '1', 'O'],\n",
       " [],\n",
       " ['В', '283', '1', 'O'],\n",
       " ['частности', '285', '9', 'O'],\n",
       " [',', '294', '1', 'O'],\n",
       " ['лидеры', '296', '6', 'O'],\n",
       " ['намерены', '303', '8', 'O'],\n",
       " ['обменяться', '312', '10', 'O'],\n",
       " ['мнениями', '323', '8', 'O'],\n",
       " ['о', '332', '1', 'O'],\n",
       " ['перспективах', '334', '12', 'O'],\n",
       " ['военных', '347', '7', 'O'],\n",
       " ['действий', '355', '8', 'O'],\n",
       " ['НАТО', '364', '4', 'S-Org'],\n",
       " ['в', '369', '1', 'O'],\n",
       " ['Афганистане', '371', '11', 'S-Location'],\n",
       " [',', '382', '1', 'O'],\n",
       " ['волнениях', '384', '9', 'O'],\n",
       " ['в', '394', '1', 'O'],\n",
       " ['Пакистане', '396', '9', 'S-Location'],\n",
       " ['и', '406', '1', 'O'],\n",
       " ['угрозах', '408', '7', 'O'],\n",
       " ['экстремистов', '416', '12', 'O'],\n",
       " ['в', '429', '1', 'O'],\n",
       " ['Северной', '431', '8', 'B-Location'],\n",
       " ['Африке', '440', '6', 'E-Location'],\n",
       " ['.', '446', '1', 'O'],\n",
       " [],\n",
       " ['В', '448', '1', 'O'],\n",
       " ['воскресенье', '450', '11', 'O'],\n",
       " ['Николя', '462', '6', 'B-Person'],\n",
       " ['Саркози', '469', '7', 'E-Person'],\n",
       " ['выпустил', '477', '8', 'O'],\n",
       " ['заявление', '486', '9', 'O'],\n",
       " ['с', '496', '1', 'O'],\n",
       " ['осуждением', '498', '10', 'O'],\n",
       " ['убийства', '509', '8', 'O'],\n",
       " ['двух', '518', '4', 'O'],\n",
       " ['французских', '523', '11', 'O'],\n",
       " ['заложников', '535', '10', 'O'],\n",
       " ['в', '546', '1', 'O'],\n",
       " ['Нигере', '548', '6', 'S-Location'],\n",
       " ['.', '554', '1', 'O'],\n",
       " [],\n",
       " ['В', '557', '1', 'O'],\n",
       " ['недавнем', '559', '8', 'O'],\n",
       " ['выступлении', '568', '11', 'O'],\n",
       " ['по', '580', '2', 'O'],\n",
       " ['французскому', '583', '12', 'O'],\n",
       " ['телевидению', '596', '11', 'O'],\n",
       " ['Саркози', '608', '7', 'S-Person'],\n",
       " ['выразил', '616', '7', 'O'],\n",
       " ['мнение', '624', '6', 'O'],\n",
       " ['о', '631', '1', 'O'],\n",
       " ['необходимости', '633', '13', 'O'],\n",
       " ['изменить', '647', '8', 'O'],\n",
       " ['ситуацию', '656', '8', 'O'],\n",
       " ['в', '665', '1', 'O'],\n",
       " ['глобальной', '667', '10', 'O'],\n",
       " ['экономике', '678', '9', 'O'],\n",
       " [',', '687', '1', 'O'],\n",
       " ['при', '689', '3', 'O'],\n",
       " ['которой', '693', '7', 'O'],\n",
       " ['доллар', '701', '6', 'O'],\n",
       " ['остается', '708', '8', 'O'],\n",
       " ['главным', '717', '7', 'O'],\n",
       " ['инструментом', '725', '12', 'O'],\n",
       " ['расчетов', '738', '8', 'O'],\n",
       " ['и', '747', '1', 'O'],\n",
       " ['мировой', '749', '7', 'O'],\n",
       " ['резервной', '757', '9', 'O'],\n",
       " ['валютой', '767', '7', 'O'],\n",
       " ['.', '774', '1', 'O'],\n",
       " [],\n",
       " ['Вашингтонская', '776', '13', 'O'],\n",
       " ['встреча', '790', '7', 'O'],\n",
       " ['проходит', '798', '8', 'O'],\n",
       " ['в', '807', '1', 'O'],\n",
       " ['канун', '809', '5', 'O'],\n",
       " ['передачи', '815', '8', 'O'],\n",
       " ['Франции', '824', '7', 'S-LocOrg'],\n",
       " ['полномочий', '832', '10', 'O'],\n",
       " ['главы', '843', '5', 'O'],\n",
       " ['«', '849', '1', 'O'],\n",
       " ['Большой', '850', '7', 'B-Org'],\n",
       " ['двадцатки', '858', '9', 'E-Org'],\n",
       " ['»', '867', '1', 'O'],\n",
       " ['.', '868', '1', 'O'],\n",
       " [],\n",
       " ['Президент', '870', '9', 'O'],\n",
       " ['Саркози', '880', '7', 'S-Person'],\n",
       " ['выразил', '888', '7', 'O'],\n",
       " ['в', '896', '1', 'O'],\n",
       " ['этой', '898', '4', 'O'],\n",
       " ['связи', '903', '5', 'O'],\n",
       " ['надежду', '909', '7', 'O'],\n",
       " ['на', '917', '2', 'O'],\n",
       " ['то', '920', '2', 'O'],\n",
       " [',', '922', '1', 'O'],\n",
       " ['что', '924', '3', 'O'],\n",
       " ['ему', '928', '3', 'O'],\n",
       " ['удастся', '932', '7', 'O'],\n",
       " ['за', '940', '2', 'O'],\n",
       " ['время', '943', '5', 'O'],\n",
       " ['председательства', '949', '16', 'O'],\n",
       " ['в', '966', '1', 'O'],\n",
       " ['G-20', '968', '4', 'S-Org'],\n",
       " ['провести', '973', '8', 'O'],\n",
       " ['целый', '982', '5', 'O'],\n",
       " ['ряд', '988', '3', 'O'],\n",
       " ['реформ', '992', '6', 'O'],\n",
       " ['.', '998', '1', 'O'],\n",
       " [],\n",
       " ['22-летний', '0', '9', 'O'],\n",
       " ['житель', '10', '6', 'O'],\n",
       " ['Аризоны', '17', '7', 'S-Location'],\n",
       " [',', '24', '1', 'O'],\n",
       " ['открывший', '26', '9', 'O'],\n",
       " ['стрельбу', '36', '8', 'O'],\n",
       " [',', '44', '1', 'O'],\n",
       " ['в', '46', '1', 'O'],\n",
       " ['результате', '48', '10', 'O'],\n",
       " ['которой', '59', '7', 'O'],\n",
       " ['пострадали', '67', '10', 'O'],\n",
       " ['20', '78', '2', 'O'],\n",
       " ['человек', '81', '7', 'O'],\n",
       " [',', '88', '1', 'O'],\n",
       " ['и', '90', '1', 'O'],\n",
       " ['попытавшийся', '92', '12', 'O'],\n",
       " ['убить', '105', '5', 'O'],\n",
       " ['члена', '111', '5', 'O'],\n",
       " ['Конгресса', '117', '9', 'B-Org'],\n",
       " ['США', '127', '3', 'E-Org'],\n",
       " [',', '130', '1', 'O'],\n",
       " ['впервые', '132', '7', 'O'],\n",
       " ['предстал', '140', '8', 'O'],\n",
       " ['перед', '149', '5', 'O'],\n",
       " ['судом', '155', '5', 'O'],\n",
       " ['по', '161', '2', 'O'],\n",
       " ['обвинению', '164', '9', 'O'],\n",
       " ['в', '174', '1', 'O'],\n",
       " ['совершении', '176', '10', 'O'],\n",
       " ['федерального', '187', '12', 'O'],\n",
       " ['преступления', '200', '12', 'O'],\n",
       " ['.', '212', '1', 'O'],\n",
       " [],\n",
       " ['В', '215', '1', 'O'],\n",
       " ['понедельник', '217', '11', 'O'],\n",
       " ['Джаред', '229', '6', 'B-Person'],\n",
       " ['Лофнер', '236', '6', 'E-Person'],\n",
       " ['в', '243', '1', 'O'],\n",
       " ['наручниках', '245', '10', 'O'],\n",
       " ['и', '256', '1', 'O'],\n",
       " ['в', '258', '1', 'O'],\n",
       " ['сопровождении', '260', '13', 'O'],\n",
       " ['нескольких', '274', '10', 'O'],\n",
       " ['охранников', '285', '10', 'O'],\n",
       " ['доставлен', '296', '9', 'O'],\n",
       " ['в', '306', '1', 'O'],\n",
       " ['суд', '308', '3', 'O'],\n",
       " ['в', '312', '1', 'O'],\n",
       " ['города', '314', '6', 'B-Location'],\n",
       " ['Феникс', '321', '6', 'E-Location'],\n",
       " ['.', '327', '1', 'O'],\n",
       " [],\n",
       " ['Побритый', '329', '8', 'O'],\n",
       " ['наголо', '338', '6', 'O'],\n",
       " ['и', '345', '1', 'O'],\n",
       " ['одетый', '347', '6', 'O'],\n",
       " ['в', '354', '1', 'O'],\n",
       " ['тюремную', '356', '8', 'O'],\n",
       " ...]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#from generator import Generator\n",
    "#from corpus import ConllCorpusReaderX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "factrueval_devset = ConllCorpusReaderX('./',\n",
    "                                       fileids='devset.txt', \n",
    "                                       columntypes=['words', 'offset', 'len', 'ne'])\n",
    "\n",
    "factrueval_testset = ConllCorpusReaderX('./', \n",
    "                                        fileids='testset.txt', \n",
    "                                        columntypes=['words', 'offset', 'len', 'ne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мэрии',\n",
       " 'Москвы',\n",
       " 'на',\n",
       " 'Тверской',\n",
       " 'площади',\n",
       " 'состоялась',\n",
       " 'очередная',\n",
       " 'несанкционированная',\n",
       " 'акция',\n",
       " 'протеста',\n",
       " '«',\n",
       " 'День',\n",
       " 'гнева',\n",
       " '»',\n",
       " ',',\n",
       " 'в',\n",
       " 'этот',\n",
       " 'раз',\n",
       " 'направленная',\n",
       " ',',\n",
       " 'главным',\n",
       " 'образом',\n",
       " ',',\n",
       " 'против']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factrueval_devset.words()[6:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('words', 'pos', 'tree', 'chunk', 'ne', 'srl', 'ignore', 'offset', 'len')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factrueval_devset.COLUMN_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(column_types=['WORD'], context_len=2)\n",
    "\n",
    "Y_train = [el[1] for el in factrueval_devset.get_ne()]\n",
    "Y_test = [el[1] for el in factrueval_testset.get_ne()] \n",
    "\n",
    "X_train = gen.fit_transform([[el] for el in factrueval_devset.words()], \n",
    "                            Y_train, \n",
    "                            path=TRAINSET_PATH)\n",
    "X_test = gen.transform([[el] for el in factrueval_testset.words()], \n",
    "                       path=TESTSET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'B-Location',\n",
       " 'E-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Location',\n",
       " 'I-Location',\n",
       " 'E-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'I-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Location',\n",
       " 'I-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Location',\n",
       " 'E-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-LocOrg',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Location',\n",
       " 'E-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'I-Org',\n",
       " 'E-Org',\n",
       " 'O',\n",
       " 'B-LocOrg',\n",
       " 'E-LocOrg',\n",
       " 'B-Person',\n",
       " 'E-Person',\n",
       " 'O',\n",
       " 'O',\n",
       " ...]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Избавляет данные от случаев O : O #\n",
    "def clean(Y_pred, Y_test):\n",
    "    Y_pred = np.array(Y_pred)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    Y_pred_i = np.array([Y_pred != 'O'])\n",
    "    Y_test_i = np.array([Y_test != 'O'])\n",
    "\n",
    "    indexes = (Y_pred_i | Y_test_i).reshape(Y_pred.shape)\n",
    "\n",
    "    Y_pred_fixed = Y_pred[indexes]\n",
    "    Y_test_fixed = Y_test[indexes]\n",
    "    return Y_pred_fixed, Y_test_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(clf=LogisticRegression()):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    Y_pred_c, Y_test_c = clean(Y_pred, Y_test)\n",
    "\n",
    "    def get_el(el):\n",
    "        if el == \"O\":\n",
    "            return el\n",
    "        else:\n",
    "            return el[2:]\n",
    "\n",
    "    Y_pred_c_light = [get_el(el) for el in Y_pred_c]\n",
    "    Y_test_c_light = [get_el(el) for el in Y_test_c]\n",
    "\n",
    "    # Strict evaluation #\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"# Strict evaluation #\")\n",
    "    counter = Counter(Y_test_c)\n",
    "    labels = list(counter.keys())\n",
    "    labels.remove(\"O\")\n",
    "    results = f1_score(Y_test_c, Y_pred_c, average=None, labels=labels)\n",
    "    for a, b in zip(labels, results):\n",
    "        print('F1 for {} == {}, with {} entities'.format(a, b, counter[a]))\n",
    "\n",
    "    print(\"Weighted Score:\", f1_score(Y_test_c, Y_pred_c, average=\"weighted\", labels=list(counter.keys())))    \n",
    "\n",
    "    # Not strict evaluation #    \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"# Not strict evaluation #\")    \n",
    "    light_counter = Counter(Y_test_c_light)\n",
    "    light_labels = list(light_counter.keys())\n",
    "    light_labels.remove(\"O\")\n",
    "    print(light_counter)\n",
    "    light_results = f1_score(Y_test_c_light, Y_pred_c_light, average=None, labels=light_labels)\n",
    "    for a, b in zip(light_labels, light_results):\n",
    "        print('F1 for {} == {}, with {} entities'.format(a, b, light_counter[a]))\n",
    "\n",
    "    print(\"Weighted Score:\", f1_score(Y_test_c_light, Y_pred_c_light, average=\"weighted\", labels=light_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for B-Person == 0.856958762886598, with 694 entities\n",
      "F1 for E-Person == 0.8702983138780804, with 692 entities\n",
      "F1 for S-Person == 0.5152027027027026, with 697 entities\n",
      "F1 for S-Location == 0.6410026857654431, with 554 entities\n",
      "F1 for B-Location == 0.28048780487804875, with 114 entities\n",
      "F1 for I-Location == 0.04705882352941177, with 74 entities\n",
      "F1 for S-Org == 0.506341463414634, with 1300 entities\n",
      "F1 for B-Org == 0.32996632996632996, with 646 entities\n",
      "F1 for I-Org == 0.29285099052540914, with 903 entities\n",
      "F1 for E-Org == 0.3525, with 600 entities\n",
      "F1 for S-LocOrg == 0.5812734082397003, with 666 entities\n",
      "F1 for I-Person == 0.15, with 27 entities\n",
      "F1 for E-Location == 0.20454545454545453, with 70 entities\n",
      "F1 for B-LocOrg == 0.3733333333333333, with 49 entities\n",
      "F1 for E-LocOrg == 0.0888888888888889, with 40 entities\n",
      "F1 for I-LocOrg == 0.0, with 13 entities\n",
      "F1 for B-Project == 0.0, with 16 entities\n",
      "F1 for I-Project == 0.0, with 12 entities\n",
      "F1 for S-Project == 0.0, with 11 entities\n",
      "F1 for E-Project == 0.0, with 15 entities\n",
      "F1 for B-Facility == 0.0, with 1 entities\n",
      "F1 for S-Facility == 0.0, with 1 entities\n",
      "Weighted Score: 0.497372000224\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'Org': 3449, 'Person': 2110, 'Location': 812, 'LocOrg': 768, 'O': 287, 'Project': 54, 'Facility': 2})\n",
      "F1 for Person == 0.8063918480778138, with 2110 entities\n",
      "F1 for Location == 0.5598349381017882, with 812 entities\n",
      "F1 for Org == 0.4724602203182374, with 3449 entities\n",
      "F1 for LocOrg == 0.5633039945836155, with 768 entities\n",
      "F1 for Project == 0.0, with 54 entities\n",
      "F1 for Facility == 0.0, with 2 entities\n",
      "Weighted Score: 0.586269011383\n"
     ]
    }
   ],
   "source": [
    "run_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for B-Person == 0.8246194573130377, with 694 entities\n",
      "F1 for E-Person == 0.8340750158931977, with 692 entities\n",
      "F1 for S-Person == 0.3465045592705167, with 697 entities\n",
      "F1 for S-Location == 0.5585903083700441, with 554 entities\n",
      "F1 for B-Location == 0.20994475138121543, with 114 entities\n",
      "F1 for I-Location == 0.10752688172043011, with 74 entities\n",
      "F1 for S-Org == 0.39633817985998926, with 1300 entities\n",
      "F1 for B-Org == 0.22222222222222224, with 646 entities\n",
      "F1 for I-Org == 0.18750000000000003, with 903 entities\n",
      "F1 for E-Org == 0.24479804161566707, with 600 entities\n",
      "F1 for S-LocOrg == 0.5, with 666 entities\n",
      "F1 for I-Person == 0.04878048780487805, with 27 entities\n",
      "F1 for E-Location == 0.04081632653061224, with 70 entities\n",
      "F1 for B-LocOrg == 0.2857142857142857, with 49 entities\n",
      "F1 for E-LocOrg == 0.1176470588235294, with 40 entities\n",
      "F1 for I-LocOrg == 0.0, with 13 entities\n",
      "F1 for B-Project == 0.0, with 16 entities\n",
      "F1 for I-Project == 0.0, with 12 entities\n",
      "F1 for S-Project == 0.0, with 11 entities\n",
      "F1 for E-Project == 0.0, with 15 entities\n",
      "F1 for B-Facility == 0.0, with 1 entities\n",
      "F1 for S-Facility == 0.0, with 1 entities\n",
      "Weighted Score: 0.404328910593\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'Org': 3449, 'Person': 2110, 'Location': 812, 'LocOrg': 768, 'O': 381, 'Project': 54, 'Facility': 2})\n",
      "F1 for Person == 0.7611867704280155, with 2110 entities\n",
      "F1 for Location == 0.4857332448573324, with 812 entities\n",
      "F1 for Org == 0.3559898045879355, with 3449 entities\n",
      "F1 for LocOrg == 0.47913862718707945, with 768 entities\n",
      "F1 for Project == 0.0, with 54 entities\n",
      "F1 for Facility == 0.0, with 2 entities\n",
      "Weighted Score: 0.499834160129\n"
     ]
    }
   ],
   "source": [
    "run_baseline(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for B-Person == 0.8586525759577279, with 694 entities\n",
      "F1 for E-Person == 0.8894771674387822, with 692 entities\n",
      "F1 for S-Person == 0.49913043478260866, with 697 entities\n",
      "F1 for S-Location == 0.6308370044052862, with 554 entities\n",
      "F1 for B-Location == 0.3186813186813187, with 114 entities\n",
      "F1 for I-Location == 0.02247191011235955, with 74 entities\n",
      "F1 for S-Org == 0.534523250352278, with 1300 entities\n",
      "F1 for B-Org == 0.3458333333333334, with 646 entities\n",
      "F1 for I-Org == 0.3472222222222222, with 903 entities\n",
      "F1 for E-Org == 0.4, with 600 entities\n",
      "F1 for S-LocOrg == 0.5823353293413174, with 666 entities\n",
      "F1 for I-Person == 0.19672131147540983, with 27 entities\n",
      "F1 for E-Location == 0.19298245614035087, with 70 entities\n",
      "F1 for B-LocOrg == 0.41758241758241765, with 49 entities\n",
      "F1 for E-LocOrg == 0.0816326530612245, with 40 entities\n",
      "F1 for I-LocOrg == 0.052631578947368425, with 13 entities\n",
      "F1 for B-Project == 0.0, with 16 entities\n",
      "F1 for I-Project == 0.0, with 12 entities\n",
      "F1 for S-Project == 0.0, with 11 entities\n",
      "F1 for E-Project == 0.0, with 15 entities\n",
      "F1 for B-Facility == 0.0, with 1 entities\n",
      "F1 for S-Facility == 0.0, with 1 entities\n",
      "Weighted Score: 0.507205553359\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'Org': 3449, 'Person': 2110, 'Location': 812, 'LocOrg': 768, 'O': 395, 'Project': 54, 'Facility': 2})\n",
      "F1 for Person == 0.8083097261567517, with 2110 entities\n",
      "F1 for Location == 0.55, with 812 entities\n",
      "F1 for Org == 0.5375, with 3449 entities\n",
      "F1 for LocOrg == 0.5627476882430646, with 768 entities\n",
      "F1 for Project == 0.0, with 54 entities\n",
      "F1 for Facility == 0.0, with 2 entities\n",
      "Weighted Score: 0.616839645137\n"
     ]
    }
   ],
   "source": [
    "run_baseline(LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for B-Person == 0.8280961182994455, with 694 entities\n",
      "F1 for E-Person == 0.81577325939618, with 692 entities\n",
      "F1 for S-Person == 0.4363636363636363, with 697 entities\n",
      "F1 for S-Location == 0.6145038167938932, with 554 entities\n",
      "F1 for B-Location == 0.16470588235294117, with 114 entities\n",
      "F1 for I-Location == 0.042105263157894736, with 74 entities\n",
      "F1 for S-Org == 0.5163814180929096, with 1300 entities\n",
      "F1 for B-Org == 0.24116424116424115, with 646 entities\n",
      "F1 for I-Org == 0.22145328719723184, with 903 entities\n",
      "F1 for E-Org == 0.20108695652173916, with 600 entities\n",
      "F1 for S-LocOrg == 0.5334252239834596, with 666 entities\n",
      "F1 for I-Person == 0.03773584905660377, with 27 entities\n",
      "F1 for E-Location == 0.13999999999999999, with 70 entities\n",
      "F1 for B-LocOrg == 0.43999999999999995, with 49 entities\n",
      "F1 for E-LocOrg == 0.1724137931034483, with 40 entities\n",
      "F1 for I-LocOrg == 0.06451612903225806, with 13 entities\n",
      "F1 for B-Project == 0.0, with 16 entities\n",
      "F1 for I-Project == 0.0, with 12 entities\n",
      "F1 for S-Project == 0.0, with 11 entities\n",
      "F1 for E-Project == 0.0, with 15 entities\n",
      "F1 for B-Facility == 0.0, with 1 entities\n",
      "F1 for S-Facility == 0.0, with 1 entities\n",
      "Weighted Score: 0.436606831131\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'Org': 3449, 'Person': 2110, 'Location': 812, 'LocOrg': 768, 'O': 475, 'Project': 54, 'Facility': 2})\n",
      "F1 for Person == 0.7656537753222836, with 2110 entities\n",
      "F1 for Location == 0.510969568294409, with 812 entities\n",
      "F1 for Org == 0.4204939783629312, with 3449 entities\n",
      "F1 for LocOrg == 0.5206463195691203, with 768 entities\n",
      "F1 for Project == 0.0, with 54 entities\n",
      "F1 for Facility == 0.0, with 2 entities\n",
      "Weighted Score: 0.539343552493\n"
     ]
    }
   ],
   "source": [
    "run_baseline(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One classifier for all classes (with prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_c, Y_test_c = clean(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strict evaluation of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for B-Person == 0.856958762886598, with 694 entities\n",
      "F1 for E-Person == 0.8702983138780804, with 692 entities\n",
      "F1 for S-Person == 0.5152027027027026, with 697 entities\n",
      "F1 for S-Location == 0.6410026857654431, with 554 entities\n",
      "F1 for B-Location == 0.28048780487804875, with 114 entities\n",
      "F1 for I-Location == 0.04705882352941177, with 74 entities\n",
      "F1 for S-Org == 0.506341463414634, with 1300 entities\n",
      "F1 for B-Org == 0.32996632996632996, with 646 entities\n",
      "F1 for I-Org == 0.29285099052540914, with 903 entities\n",
      "F1 for E-Org == 0.3525, with 600 entities\n",
      "F1 for S-LocOrg == 0.5812734082397003, with 666 entities\n",
      "F1 for I-Person == 0.15, with 27 entities\n",
      "F1 for E-Location == 0.20454545454545453, with 70 entities\n",
      "F1 for B-LocOrg == 0.3733333333333333, with 49 entities\n",
      "F1 for E-LocOrg == 0.0888888888888889, with 40 entities\n",
      "F1 for I-LocOrg == 0.0, with 13 entities\n",
      "F1 for B-Project == 0.0, with 16 entities\n",
      "F1 for I-Project == 0.0, with 12 entities\n",
      "F1 for S-Project == 0.0, with 11 entities\n",
      "F1 for E-Project == 0.0, with 15 entities\n",
      "F1 for B-Facility == 0.0, with 1 entities\n",
      "F1 for S-Facility == 0.0, with 1 entities\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(Y_test_c)\n",
    "labels = list(counter.keys())\n",
    "labels.remove(\"O\")\n",
    "results = f1_score(Y_test_c, Y_pred_c, average=None, labels=labels)\n",
    "for a, b in zip(labels, results):\n",
    "    print('F1 for {} == {}, with {} entities'.format(a, b, counter[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.497372000224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f1_score(Y_test_c, Y_pred_c, average=\"weighted\", labels=list(counter.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not strict evaluation of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_el(el):\n",
    "    if el == \"O\":\n",
    "        return el\n",
    "    else:\n",
    "        return el[2:]\n",
    "    \n",
    "Y_pred_c_light = [get_el(el) for el in Y_pred_c]\n",
    "Y_test_c_light = [get_el(el) for el in Y_test_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Org': 3449, 'Person': 2110, 'Location': 812, 'LocOrg': 768, 'O': 287, 'Project': 54, 'Facility': 2})\n",
      "F1 for Person == 0.8063918480778138, with 2110 entities\n",
      "F1 for Location == 0.5598349381017882, with 812 entities\n",
      "F1 for Org == 0.4724602203182374, with 3449 entities\n",
      "F1 for LocOrg == 0.5633039945836155, with 768 entities\n",
      "F1 for Project == 0.0, with 54 entities\n",
      "F1 for Facility == 0.0, with 2 entities\n"
     ]
    }
   ],
   "source": [
    "light_counter = Counter(Y_test_c_light)\n",
    "light_labels = list(light_counter.keys())\n",
    "light_labels.remove(\"O\")\n",
    "print(light_counter)\n",
    "light_results = f1_score(Y_test_c_light, Y_pred_c_light, average=None, labels=light_labels)\n",
    "for a, b in zip(light_labels, light_results):\n",
    "    print('F1 for {} == {}, with {} entities'.format(a, b, light_counter[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586269011383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f1_score(Y_test_c_light, Y_pred_c_light, average=\"weighted\", labels=light_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One classifier for all classes (without prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_el(el):\n",
    "    if el == \"O\":\n",
    "        return el\n",
    "    else:\n",
    "        return el[2:]\n",
    "\n",
    "Y_train = [get_el(el[1]) for el in factrueval_devset.get_ne()]\n",
    "Y_test = [get_el(el[1]) for el in factrueval_testset.get_ne()] \n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "Y_pred_c, Y_test_c = clean(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not strict evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Org': 3449, 'Person': 2110, 'Location': 812, 'LocOrg': 768, 'O': 343, 'Project': 54, 'Facility': 2})\n",
      "F1 for Person == 0.8133906356028048, with 2110 entities\n",
      "F1 for O == 0.0, with 343 entities\n",
      "F1 for Location == 0.573170731707317, with 812 entities\n",
      "F1 for Org == 0.5266445442123756, with 3449 entities\n",
      "F1 for LocOrg == 0.5811965811965812, with 768 entities\n",
      "F1 for Project == 0.0, with 54 entities\n",
      "F1 for Facility == 0.0, with 2 entities\n"
     ]
    }
   ],
   "source": [
    "light_counter = Counter(Y_test_c)\n",
    "light_labels = list(light_counter.keys())\n",
    "print(light_counter)\n",
    "light_results = f1_score(Y_test_c, Y_pred_c, average=None, labels=light_labels)\n",
    "for a, b in zip(light_labels, light_results):\n",
    "    print('F1 for {} == {}, with {} entities'.format(a, b, light_counter[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589602664184\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(Y_test_c, Y_pred_c, average=\"weighted\", labels=light_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different classifiers for different classes (without prefixes and with prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diff_classes(template, prefixes=False):\n",
    "    def get_only(el):\n",
    "        if (el[2:] == template):\n",
    "            return el[2:]\n",
    "        else:\n",
    "            return \"O\"\n",
    "        \n",
    "    Y_train = [get_only(el[1]) for el in factrueval_devset.get_ne()]\n",
    "    Y_test = [get_only(el[1]) for el in factrueval_testset.get_ne()] \n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "\n",
    "    Y_pred_c, Y_test_c = clean(Y_pred, Y_test)\n",
    "    \n",
    "    light_counter = Counter(Y_test_c)\n",
    "    light_counter_2 = Counter(Y_pred_c)\n",
    "    labels = list(light_counter.keys())\n",
    "    labels.remove(\"O\")\n",
    "    print(labels)\n",
    "    light_result = f1_score(Y_test_c, Y_pred_c, average=\"binary\", pos_label=template)\n",
    "    print('F1 for {} == {}, with {} entities'.format(template, light_result, light_counter[template]))\n",
    "        \n",
    "    return light_result, light_counter[template]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Person']\n",
      "F1 for Person == 0.7966682998530131, with 2110 entities\n"
     ]
    }
   ],
   "source": [
    "result1, weight1 = run_diff_classes(\"Person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Org']\n",
      "F1 for Org == 0.4326861531896736, with 3449 entities\n"
     ]
    }
   ],
   "source": [
    "result2, weight2 = run_diff_classes(\"Org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LocOrg']\n",
      "F1 for LocOrg == 0.5443686006825939, with 768 entities\n"
     ]
    }
   ],
   "source": [
    "result3, weight3 = run_diff_classes(\"LocOrg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Location']\n",
      "F1 for Location == 0.5385202135774217, with 812 entities\n"
     ]
    }
   ],
   "source": [
    "result4, weight4 = run_diff_classes(\"Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_weight = weight1 + weight2 + weight3 + weight4\n",
    "total_result = (result1 * weight1 + result2 * weight2 + result3 * weight3 + result4 * weight4) / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.564316872642\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(total_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
