{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINSET_PATH = \"./factrueval_trainset.npz\"\n",
    "TESTSET_PATH = \"./factrueval_testset.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Какие-то классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymorphy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-de5960ddd271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymorphy2'"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Generator:\n",
    "\n",
    "    def __init__(self,\n",
    "                 column_types=None,\n",
    "                 context_len=2,\n",
    "                 language='ru',\n",
    "                 number_of_occurences=5,\n",
    "                 weight_percentage=0.9):\n",
    "\n",
    "        # Частота, ниже которой лейбл считается \"редким\" #\n",
    "        self.NUMBER_OF_OCCURENCES = number_of_occurences\n",
    "\n",
    "        # Процент веса признаков, который нужно оставить\n",
    "        self.WEIGHT_PERCENTAGE = weight_percentage  #\n",
    "\n",
    "        # Информация о подаваемых столбцах (может быть WORD, POS, CHUNK) #\n",
    "        self._column_types = column_types if column_types is not None else [\"WORD\"]\n",
    "\n",
    "        # Длина рассматриваемого контекста (context_len влево и context_len вправо) #\n",
    "        self._context_len = context_len\n",
    "\n",
    "        # Анализатор (для POS-тега и начальной формы) #\n",
    "        self._morph = pymorphy2.MorphAnalyzer()\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Язык датасета (определяет используемые модули) #\n",
    "        self._lang = language\n",
    "\n",
    "        # OneHotEncoder, хранится после FIT-а #\n",
    "        self._enc = None\n",
    "\n",
    "        # ColumnApplier, хранится после FIT-а #\n",
    "        self._multi_encoder = None\n",
    "\n",
    "        # Словари распознаваемых слов, хранятся после FIT-а #\n",
    "        self._counters = []\n",
    "\n",
    "        # Число столбцов в \"сырой\" матрице признаков #\n",
    "        self._number_of_columns = None\n",
    "\n",
    "        # Индексы столбцов признаков, оставленных после отсева #\n",
    "        self._columns_to_keep = None\n",
    "\n",
    "    def fit_transform(self, data, answers, path, clf=ExtraTreesClassifier()):\n",
    "\n",
    "        # Eсли данные сохранены - просто берем их из файла #\n",
    "        if os.path.exists(path):\n",
    "            sparse_features_list = self.load_sparse_csr(path)\n",
    "            return sparse_features_list\n",
    "\n",
    "        # Добавляем пустые \"слова\" в начало и конец (для контекста) #\n",
    "        data = [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)] + data\n",
    "        data = data + [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)]\n",
    "\n",
    "        # Находим индексы столбцов в переданных данных #\n",
    "        word_index = self._column_types.index(\"WORD\")\n",
    "        if \"POS\" in self._column_types:\n",
    "            pos_index = self._column_types.index(\"POS\")\n",
    "        else:\n",
    "            pos_index = None\n",
    "        if \"POS\" in self._column_types:\n",
    "            chunk_index = self._column_types.index(\"CHUNK\")\n",
    "        else:\n",
    "            chunk_index = None\n",
    "\n",
    "        # Список признаков (строка == набор признаков для слова из массива data) #\n",
    "        features_list = []\n",
    "\n",
    "        # Заполнение массива features_list \"сырыми\" данными (без отсева) #\n",
    "        for k in range(len(data) - 2 * self._context_len):\n",
    "            arr = []\n",
    "            i = k + self._context_len\n",
    "\n",
    "            if pos_index is not None:\n",
    "                pos_arr = [data[i][pos_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(data[i - j][pos_index])\n",
    "                    pos_arr.append(data[i + j][pos_index])\n",
    "            else:\n",
    "                pos_arr = [self.get_pos_tag(data[i][word_index])]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(self.get_pos_tag(data[i - j][word_index]))\n",
    "                    pos_arr.append(self.get_pos_tag(data[i + j][word_index]))\n",
    "            arr += pos_arr\n",
    "\n",
    "            if chunk_index is not None:\n",
    "                chunk_arr = [data[i][chunk_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    chunk_arr.append(data[i - j][chunk_index])\n",
    "                    chunk_arr.append(data[i + j][chunk_index])\n",
    "                arr += chunk_arr\n",
    "\n",
    "            capital_arr = [self.get_capital(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                capital_arr.append(self.get_capital(data[i - j][word_index]))\n",
    "                capital_arr.append(self.get_capital(data[i + j][word_index]))\n",
    "            arr += capital_arr\n",
    "\n",
    "            is_punct_arr = [self.get_is_punct(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_punct_arr.append(self.get_is_punct(data[i - j][word_index]))\n",
    "                is_punct_arr.append(self.get_is_punct(data[i + j][word_index]))\n",
    "            arr += is_punct_arr\n",
    "\n",
    "            is_number_arr = [self.get_is_number(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_number_arr.append(self.get_is_number(data[i - j][word_index]))\n",
    "                is_number_arr.append(self.get_is_number(data[i + j][word_index]))\n",
    "            arr += is_number_arr\n",
    "\n",
    "            initial_arr = [self.get_initial(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                initial_arr.append(self.get_initial(data[i - j][word_index]))\n",
    "                initial_arr.append(self.get_initial(data[i + j][word_index]))\n",
    "            arr += initial_arr\n",
    "\n",
    "            features_list.append(arr)\n",
    "\n",
    "        # Теперь это массив сырых признаков (в строковом представлении, без отсева) #\n",
    "        features_list = np.array([np.array(line) for line in features_list])\n",
    "\n",
    "        # Выкинем из этого массива классы, встретившиеся менее NUMBER_OF_OCCURENCES раз #\n",
    "        # Посчитаем частоту лейблов в столбце #\n",
    "        self._number_of_columns = features_list.shape[1]\n",
    "        for u in range(self._number_of_columns):\n",
    "            arr = features_list[:, u]\n",
    "            counter = Counter(arr)\n",
    "            self._counters.append(counter)\n",
    "\n",
    "        # Избавимся от редких лейблов (частота < NUMBER_OF_OCC) #\n",
    "        for y in range(len(features_list)):\n",
    "            for x in range(self._number_of_columns):\n",
    "                features_list[y][x] = self.get_feature(x, features_list[y][x])\n",
    "\n",
    "        # Оставшиеся признаки бинаризуем #\n",
    "        self._multi_encoder = ColumnApplier(\n",
    "            dict([(i, preprocessing.LabelEncoder()) for i in range(len(features_list[0]))]))\n",
    "        features_list = self._multi_encoder.fit(features_list, None).transform(features_list)\n",
    "        self._enc = preprocessing.OneHotEncoder(dtype=np.bool_, sparse=True)\n",
    "        self._enc.fit(features_list)\n",
    "        features_list = self._enc.transform(features_list)\n",
    "\n",
    "        # Избавляемся от неинформативных признаков (WEIGHT = WEIGHT_PERC * TOTAL_WEIGHT)#\n",
    "        clf.fit(features_list, answers)\n",
    "        features_importances = [(i, el) for i, el in enumerate(clf.feature_importances_)]\n",
    "\n",
    "        features_importances = sorted(features_importances, key=lambda el: -el[1])\n",
    "        current_weight = 0.0\n",
    "        self._columns_to_keep = []\n",
    "        for el in features_importances:\n",
    "            self._columns_to_keep.append(el[0])\n",
    "            current_weight += el[1]\n",
    "            if current_weight > self.WEIGHT_PERCENTAGE:\n",
    "                break\n",
    "\n",
    "        features_list = features_list[:, self._columns_to_keep]\n",
    "\n",
    "        # Сохраняем матрицу в файл #\n",
    "        self.save_sparse_csr(path, features_list)\n",
    "\n",
    "        # Возвращаем матрицу #\n",
    "        return features_list\n",
    "\n",
    "    def transform(self, data, path):\n",
    "\n",
    "        # Eсли данные сохранены - просто берем их из файла #\n",
    "        if os.path.exists(path):\n",
    "            sparse_features_list = self.load_sparse_csr(path)\n",
    "            return sparse_features_list\n",
    "\n",
    "        # Добавляем пустые \"слова\" в начало и конец (для контекста) #\n",
    "        data = [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)] + data\n",
    "        data = data + [[\"\" for i in range(len(self._column_types))] for i in range(self._context_len)]\n",
    "\n",
    "        # Находим индексы столбцов в переданных данных #\n",
    "        word_index = self._column_types.index(\"WORD\")\n",
    "        if \"POS\" in self._column_types:\n",
    "            pos_index = self._column_types.index(\"POS\")\n",
    "        else:\n",
    "            pos_index = None\n",
    "        if \"CHUNK\" in self._column_types:\n",
    "            chunk_index = self._column_types.index(\"CHUNK\")\n",
    "        else:\n",
    "            chunk_index = None\n",
    "\n",
    "        # Список признаков (строка == набор признаков для слова из массива data) #\n",
    "        features_list = []\n",
    "\n",
    "        # Заполнение массива features_list \"сырыми\" данными (без отсева) #\n",
    "        for k in range(len(data) - 2 * self._context_len):\n",
    "            arr = []\n",
    "            i = k + self._context_len\n",
    "\n",
    "            if pos_index is not None:\n",
    "                pos_arr = [data[i][pos_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(data[i - j][pos_index])\n",
    "                    pos_arr.append(data[i + j][pos_index])\n",
    "            else:\n",
    "                pos_arr = [self.get_pos_tag(data[i][word_index])]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    pos_arr.append(self.get_pos_tag(data[i - j][word_index]))\n",
    "                    pos_arr.append(self.get_pos_tag(data[i + j][word_index]))\n",
    "            arr += pos_arr\n",
    "\n",
    "            if chunk_index is not None:\n",
    "                chunk_arr = [data[i][chunk_index]]\n",
    "                for j in range(1, self._context_len + 1):\n",
    "                    chunk_arr.append(data[i - j][chunk_index])\n",
    "                    chunk_arr.append(data[i + j][chunk_index])\n",
    "                arr += chunk_arr\n",
    "\n",
    "            capital_arr = [self.get_capital(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                capital_arr.append(self.get_capital(data[i - j][word_index]))\n",
    "                capital_arr.append(self.get_capital(data[i + j][word_index]))\n",
    "            arr += capital_arr\n",
    "\n",
    "            is_punct_arr = [self.get_is_punct(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_punct_arr.append(self.get_is_punct(data[i - j][word_index]))\n",
    "                is_punct_arr.append(self.get_is_punct(data[i + j][word_index]))\n",
    "            arr += is_punct_arr\n",
    "\n",
    "            is_number_arr = [self.get_is_number(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                is_number_arr.append(self.get_is_number(data[i - j][word_index]))\n",
    "                is_number_arr.append(self.get_is_number(data[i + j][word_index]))\n",
    "            arr += is_number_arr\n",
    "\n",
    "            initial_arr = [self.get_initial(data[i][word_index])]\n",
    "            for j in range(1, self._context_len + 1):\n",
    "                initial_arr.append(self.get_initial(data[i - j][word_index]))\n",
    "                initial_arr.append(self.get_initial(data[i + j][word_index]))\n",
    "            arr += initial_arr\n",
    "\n",
    "            features_list.append(arr)\n",
    "\n",
    "        # Теперь это массив сырых признаков (в строковом представлении, без отсева) #\n",
    "        features_list = np.array([np.array(line) for line in features_list])\n",
    "\n",
    "        # Выкинем из этого массива классы, встретившиеся менее NUMBER_OF_OCCURENCES раз #\n",
    "        self._number_of_columns = features_list.shape[1]\n",
    "        for y in range(len(features_list)):\n",
    "            for x in range(self._number_of_columns):\n",
    "                features_list[y][x] = self.get_feature(x, features_list[y][x])\n",
    "\n",
    "        # Оставшиеся признаки бинаризуем #\n",
    "        features_list = self._multi_encoder.transform(features_list)\n",
    "        features_list = self._enc.transform(features_list)\n",
    "\n",
    "        # Избавляемся от неинформативных признаков (WEIGHT = WEIGHT_PERC * TOTAL_WEIGHT)#\n",
    "        features_list = features_list[:, self._columns_to_keep]\n",
    "\n",
    "        # Сохраняем матрицу в файл #\n",
    "        self.save_sparse_csr(path, features_list)\n",
    "\n",
    "        # Возвращаем матрицу #\n",
    "        return features_list\n",
    "\n",
    "    # Заменяет лейбл на \"*\", если он \"редкий\" #\n",
    "    def get_feature(self, f, feature):\n",
    "        if feature in self._counters[f].keys() and self._counters[f][feature] > self.NUMBER_OF_OCCURENCES:\n",
    "            return feature\n",
    "        else:\n",
    "            return \"*\"\n",
    "\n",
    "    # Сохраняет матрицу в файл #\n",
    "    def save_sparse_csr(self, filename, array):\n",
    "        np.savez(filename,\n",
    "                 data=array.data,\n",
    "                 indices=array.indices,\n",
    "                 indptr=array.indptr,\n",
    "                 shape=array.shape)\n",
    "\n",
    "    # Загружает матрицу из файла #\n",
    "    def load_sparse_csr(self, filename):\n",
    "        loader = np.load(filename)\n",
    "        return csr_matrix((loader['data'],\n",
    "                           loader['indices'],\n",
    "                           loader['indptr']),\n",
    "                          shape=loader['shape'])\n",
    "\n",
    "    # Возвращает POS-тег слова #\n",
    "    def get_pos_tag(self, token):\n",
    "        if self._lang == 'ru':\n",
    "            pos = self._morph.parse(token)[0].tag.POS\n",
    "        else:\n",
    "            pos = None\n",
    "        if pos is not None:\n",
    "            return pos\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    # Возвращает тип регистра слова #\n",
    "    def get_capital(self, token):\n",
    "        pattern = re.compile(\"[{}]+$\".format(re.escape(string.punctuation)))\n",
    "        if pattern.match(token):\n",
    "            return \"none\"\n",
    "        if len(token) == 0:\n",
    "            return \"none\"\n",
    "        if token.islower():\n",
    "            return \"lower\"\n",
    "        elif token.isupper():\n",
    "            return \"upper\"\n",
    "        elif token[0].isupper() and len(token) == 1:\n",
    "            return \"proper\"\n",
    "        elif token[0].isupper() and token[1:].islower():\n",
    "            return \"proper\"\n",
    "        else:\n",
    "            return \"camel\"\n",
    "\n",
    "    # Признак того, является ли слово числом #\n",
    "    def get_is_number(self, token):\n",
    "        try:\n",
    "            complex(token)\n",
    "        except ValueError:\n",
    "            return \"no\"\n",
    "        return \"yes\"\n",
    "\n",
    "    # Возвращает начальную форму слова #\n",
    "    def get_initial(self, token):\n",
    "        if self._lang == 'ru':\n",
    "            initial = self._morph.parse(token)[0].normal_form\n",
    "        else:\n",
    "            initial = self._lemmatizer.lemmatize(token)\n",
    "\n",
    "        if initial is not None:\n",
    "            return initial\n",
    "        else:\n",
    "            return \"none\"\n",
    "\n",
    "    # Признак того, является ли слово пунктуацией #\n",
    "    def get_is_punct(self, token):\n",
    "        pattern = re.compile(\"[{}]+$\".format(re.escape(string.punctuation)))\n",
    "        if pattern.match(token):\n",
    "            return \"yes\"\n",
    "        else:\n",
    "            return \"no\"\n",
    "\n",
    "\n",
    "# Переводит категории в числовое представление #\n",
    "class ColumnApplier(object):\n",
    "    def __init__(self, column_stages):\n",
    "        self._column_stages = column_stages\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for i, k in self._column_stages.items():\n",
    "            k.fit(x[:, i])\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        for i, k in self._column_stages.items():\n",
    "            x[:, i] = k.transform(x[:, i])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import textwrap\n",
    "\n",
    "from six import string_types\n",
    "\n",
    "from nltk import compat\n",
    "from nltk.tree import Tree\n",
    "from nltk.util import LazyMap, LazyConcatenation\n",
    "from nltk.tag import map_tag\n",
    "\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.corpus.reader.api import *\n",
    "\n",
    "class ConllCorpusReaderX(CorpusReader):\n",
    "\n",
    "    WORDS = 'words'   #: column type for words\n",
    "    POS = 'pos'       #: column type for part-of-speech tags\n",
    "    TREE = 'tree'     #: column type for parse trees\n",
    "    CHUNK = 'chunk'   #: column type for chunk structures\n",
    "    NE = 'ne'         #: column type for named entities\n",
    "    SRL = 'srl'       #: column type for semantic role labels\n",
    "    IGNORE = 'ignore' #: column type for column that should be ignored\n",
    "    OFFSET = 'offset'\n",
    "    LEN = 'len'\n",
    "\n",
    "    #: A list of all column types supported by the conll corpus reader.\n",
    "    COLUMN_TYPES = (WORDS, POS, TREE, CHUNK, NE, SRL, IGNORE, OFFSET, LEN)\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Constructor\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "\n",
    "    def __init__(self, root, fileids, columntypes,\n",
    "                 chunk_types=None, root_label='S', pos_in_tree=False,\n",
    "                 srl_includes_roleset=True, encoding='utf8',\n",
    "                 tree_class=Tree, tagset=None):\n",
    "        for columntype in columntypes:\n",
    "            if columntype not in self.COLUMN_TYPES:\n",
    "                raise ValueError('Bad column type %r' % columntype)\n",
    "        if isinstance(chunk_types, string_types):\n",
    "            chunk_types = [chunk_types]\n",
    "        self._chunk_types = chunk_types\n",
    "        self._colmap = dict((c,i) for (i,c) in enumerate(columntypes))\n",
    "        self._pos_in_tree = pos_in_tree\n",
    "        self._root_label = root_label # for chunks\n",
    "        self._srl_includes_roleset = srl_includes_roleset\n",
    "        self._tree_class = tree_class\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        self._tagset = tagset\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Data Access Methods\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "\n",
    "    def raw(self, fileids=None):\n",
    "        if fileids is None: fileids = self._fileids\n",
    "        elif isinstance(fileids, string_types): fileids = [fileids]\n",
    "        return concat([self.open(f).read() for f in fileids])\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        self._require(self.WORDS)\n",
    "        return LazyConcatenation(LazyMap(self._get_words, self._grids(fileids)))\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        self._require(self.WORDS)\n",
    "        return LazyMap(self._get_words, self._grids(fileids))\n",
    "\n",
    "    def tagged_words(self, fileids=None, tagset=None):\n",
    "        self._require(self.WORDS, self.POS)\n",
    "        def get_tagged_words(grid):\n",
    "            return self._get_tagged_words(grid, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_tagged_words,\n",
    "                                         self._grids(fileids)))\n",
    "\n",
    "    def tagged_sents(self, fileids=None, tagset=None):\n",
    "        self._require(self.WORDS, self.POS)\n",
    "        def get_tagged_words(grid):\n",
    "            return self._get_tagged_words(grid, tagset)\n",
    "        return LazyMap(get_tagged_words, self._grids(fileids))\n",
    "\n",
    "    def chunked_words(self, fileids=None, chunk_types=None,\n",
    "                      tagset=None):\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        if chunk_types is None: chunk_types = self._chunk_types\n",
    "        def get_chunked_words(grid): # capture chunk_types as local var\n",
    "            return self._get_chunked_words(grid, chunk_types, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_chunked_words,\n",
    "                                         self._grids(fileids)))\n",
    "    \n",
    "    def get_tags(self, fileids=None, tagset=None, tags=[]):\n",
    "        required = []\n",
    "        for tag in tags:\n",
    "            if tag == 'offset':\n",
    "                required.append(self.OFFSET)\n",
    "            if tag == 'len':\n",
    "                required.append(self.LEN)\n",
    "            if tag == 'words':\n",
    "                required.append(self.WORDS)\n",
    "            if tag == 'pos':\n",
    "                required.append(self.POS)\n",
    "            if tag == 'tree':\n",
    "                required.append(self.TREE)\n",
    "            if tag == 'ne':\n",
    "                required.append(self.NE)\n",
    "            if tag == 'srl':\n",
    "                required.append(self.SRL)\n",
    "            if tag == 'ignore':\n",
    "                required.append(self.IGNORE)\n",
    "            if tag == 'chunk':\n",
    "                required.append(self.CHUNK)\n",
    "\n",
    "        self._require(*required)\n",
    "        def get_tags_inn(grid, tags=tags):\n",
    "            return self._get_tags(grid, tagset, tags=tags)\n",
    "        return LazyConcatenation(LazyMap(get_tags_inn, self._grids(fileids)))\n",
    "    \n",
    "    def _get_tags(self, grid, tagset=None, tags=None):\n",
    "        columns = [self._get_column(grid, self._colmap[tag]) for tag in tags]\n",
    "        return list(zip(*columns))\n",
    "    \n",
    "\n",
    "    def chunked_sents(self, fileids=None, chunk_types=None,\n",
    "                      tagset=None):\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        if chunk_types is None: chunk_types = self._chunk_types\n",
    "        def get_chunked_words(grid): # capture chunk_types as local var\n",
    "            return self._get_chunked_words(grid, chunk_types, tagset)\n",
    "        return LazyMap(get_chunked_words, self._grids(fileids))\n",
    "\n",
    "    def parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None):\n",
    "        self._require(self.WORDS, self.POS, self.TREE)\n",
    "        if pos_in_tree is None: pos_in_tree = self._pos_in_tree\n",
    "        def get_parsed_sent(grid): # capture pos_in_tree as local var\n",
    "            return self._get_parsed_sent(grid, pos_in_tree, tagset)\n",
    "        return LazyMap(get_parsed_sent, self._grids(fileids))\n",
    "\n",
    "    def srl_spans(self, fileids=None):\n",
    "        self._require(self.SRL)\n",
    "        return LazyMap(self._get_srl_spans, self._grids(fileids))\n",
    "\n",
    "    def srl_instances(self, fileids=None, pos_in_tree=None, flatten=True):\n",
    "        self._require(self.WORDS, self.POS, self.TREE, self.SRL)\n",
    "        if pos_in_tree is None: pos_in_tree = self._pos_in_tree\n",
    "        def get_srl_instances(grid): # capture pos_in_tree as local var\n",
    "            return self._get_srl_instances(grid, pos_in_tree)\n",
    "        result = LazyMap(get_srl_instances, self._grids(fileids))\n",
    "        if flatten: result = LazyConcatenation(result)\n",
    "        return result\n",
    "\n",
    "    def iob_words(self, fileids=None, tagset=None):\n",
    "        \"\"\"\n",
    "        :return: a list of word/tag/IOB tuples\n",
    "        :rtype: list(tuple)\n",
    "        :param fileids: the list of fileids that make up this corpus\n",
    "        :type fileids: None or str or list\n",
    "        \"\"\"\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        def get_iob_words(grid):\n",
    "            return self._get_iob_words(grid, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_iob_words, self._grids(fileids)))\n",
    "\n",
    "    def iob_sents(self, fileids=None, tagset=None):\n",
    "        \"\"\"\n",
    "        :return: a list of lists of word/tag/IOB tuples\n",
    "        :rtype: list(list)\n",
    "        :param fileids: the list of fileids that make up this corpus\n",
    "        :type fileids: None or str or list\n",
    "        \"\"\"\n",
    "        self._require(self.WORDS, self.POS, self.CHUNK)\n",
    "        def get_iob_words(grid):\n",
    "            return self._get_iob_words(grid, tagset)\n",
    "        return LazyMap(get_iob_words, self._grids(fileids))\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Grid Reading\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "\n",
    "    def _grids(self, fileids=None):\n",
    "        # n.b.: we could cache the object returned here (keyed on\n",
    "        # fileids), which would let us reuse the same corpus view for\n",
    "        # different things (eg srl and parse trees).\n",
    "        return concat([StreamBackedCorpusView(fileid, self._read_grid_block,\n",
    "                                              encoding=enc)\n",
    "                       for (fileid, enc) in self.abspaths(fileids, True)])\n",
    "\n",
    "    def _read_grid_block(self, stream):\n",
    "        grids = []\n",
    "        for block in read_blankline_block(stream):\n",
    "            block = block.strip()\n",
    "            if not block: continue\n",
    "\n",
    "            grid = [line.split() for line in block.split('\\n')]\n",
    "\n",
    "            # If there's a docstart row, then discard. ([xx] eventually it\n",
    "            # would be good to actually use it)\n",
    "            if grid[0][self._colmap.get('words', 0)] == '-DOCSTART-':\n",
    "                del grid[0]\n",
    "\n",
    "            # Check that the grid is consistent.\n",
    "            for row in grid:\n",
    "                if len(row) != len(grid[0]):\n",
    "                    raise ValueError('Inconsistent number of columns:\\n%s'\n",
    "                                     % block)\n",
    "            grids.append(grid)\n",
    "        return grids\n",
    "\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # Transforms\n",
    "    #/////////////////////////////////////////////////////////////////\n",
    "    # given a grid, transform it into some representation (e.g.,\n",
    "    # a list of words or a parse tree).\n",
    "\n",
    "    def _get_words(self, grid):\n",
    "        return self._get_column(grid, self._colmap['words'])\n",
    "\n",
    "    def _get_tagged_words(self, grid, tagset=None):\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags))\n",
    "\n",
    "    def _get_iob_words(self, grid, tagset=None):\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags,\n",
    "                   self._get_column(grid, self._colmap['chunk'])))\n",
    "\n",
    "    def _get_chunked_words(self, grid, chunk_types, tagset=None):\n",
    "        # n.b.: this method is very similar to conllstr2tree.\n",
    "        words = self._get_column(grid, self._colmap['words'])\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        chunk_tags = self._get_column(grid, self._colmap['chunk'])\n",
    "\n",
    "        stack = [Tree(self._root_label, [])]\n",
    "\n",
    "        for (word, pos_tag, chunk_tag) in zip(words, pos_tags, chunk_tags):\n",
    "            if chunk_tag == 'O':\n",
    "                state, chunk_type = 'O', ''\n",
    "            else:\n",
    "                (state, chunk_type) = chunk_tag.split('-')\n",
    "            # If it's a chunk we don't care about, treat it as O.\n",
    "            if chunk_types is not None and chunk_type not in chunk_types:\n",
    "                state = 'O'\n",
    "            # Treat a mismatching I like a B.\n",
    "            if state == 'I' and chunk_type != stack[-1].label():\n",
    "                state = 'B'\n",
    "            # For B or I: close any open chunks\n",
    "            if state in 'BO' and len(stack) == 2:\n",
    "                stack.pop()\n",
    "            # For B: start a new chunk.\n",
    "            if state == 'B':\n",
    "                new_chunk = Tree(chunk_type, [])\n",
    "                stack[-1].append(new_chunk)\n",
    "                stack.append(new_chunk)\n",
    "            # Add the word token.\n",
    "            stack[-1].append((word, pos_tag))\n",
    "\n",
    "        return stack[0]\n",
    "\n",
    "    def _get_parsed_sent(self, grid, pos_in_tree, tagset=None):\n",
    "        words = self._get_column(grid, self._colmap['words'])\n",
    "        pos_tags = self._get_column(grid, self._colmap['pos'])\n",
    "        if tagset and tagset != self._tagset:\n",
    "            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n",
    "        parse_tags = self._get_column(grid, self._colmap['tree'])\n",
    "\n",
    "        treestr = ''\n",
    "        for (word, pos_tag, parse_tag) in zip(words, pos_tags, parse_tags):\n",
    "            if word == '(': word = '-LRB-'\n",
    "            if word == ')': word = '-RRB-'\n",
    "            if pos_tag == '(': pos_tag = '-LRB-'\n",
    "            if pos_tag == ')': pos_tag = '-RRB-'\n",
    "            (left, right) = parse_tag.split('*')\n",
    "            right = right.count(')')*')' # only keep ')'.\n",
    "            treestr += '%s (%s %s) %s' % (left, pos_tag, word, right)\n",
    "        try:\n",
    "            tree = self._tree_class.fromstring(treestr)\n",
    "        except (ValueError, IndexError):\n",
    "            tree = self._tree_class.fromstring('(%s %s)' %\n",
    "                                          (self._root_label, treestr))\n",
    "\n",
    "        if not pos_in_tree:\n",
    "            for subtree in tree.subtrees():\n",
    "                for i, child in enumerate(subtree):\n",
    "                    if (isinstance(child, Tree) and len(child)==1 and\n",
    "                        isinstance(child[0], string_types)):\n",
    "                        subtree[i] = (child[0], child.label())\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def _get_srl_spans(self, grid):\n",
    "        \"\"\"\n",
    "        list of list of (start, end), tag) tuples\n",
    "        \"\"\"\n",
    "        if self._srl_includes_roleset:\n",
    "            predicates = self._get_column(grid, self._colmap['srl']+1)\n",
    "            start_col = self._colmap['srl']+2\n",
    "        else:\n",
    "            predicates = self._get_column(grid, self._colmap['srl'])\n",
    "            start_col = self._colmap['srl']+1\n",
    "\n",
    "        # Count how many predicates there are.  This tells us how many\n",
    "        # columns to expect for SRL data.\n",
    "        num_preds = len([p for p in predicates if p != '-'])\n",
    "\n",
    "        spanlists = []\n",
    "        for i in range(num_preds):\n",
    "            col = self._get_column(grid, start_col+i)\n",
    "            spanlist = []\n",
    "            stack = []\n",
    "            for wordnum, srl_tag in enumerate(col):\n",
    "                (left, right) = srl_tag.split('*')\n",
    "                for tag in left.split('('):\n",
    "                    if tag:\n",
    "                        stack.append((tag, wordnum))\n",
    "                for i in range(right.count(')')):\n",
    "                    (tag, start) = stack.pop()\n",
    "                    spanlist.append( ((start, wordnum+1), tag) )\n",
    "            spanlists.append(spanlist)\n",
    "\n",
    "        return spanlists\n",
    "    \n",
    "    def get_ne(self, fileids=None, tagset=None):\n",
    "        self._require(self.NE)\n",
    "        def get_ne_inn(grid):\n",
    "            return self._get_ne(grid, tagset)\n",
    "        return LazyConcatenation(LazyMap(get_ne_inn, self._grids(fileids)))\n",
    "    \n",
    "    def _get_ne(self, grid, tagset=None):\n",
    "        return list(zip(self._get_column(grid, self._colmap['words']),\n",
    "                        self._get_column(grid, self._colmap['ne'])))\n",
    "    \n",
    "    def _get_srl_instances(self, grid, pos_in_tree):\n",
    "        tree = self._get_parsed_sent(grid, pos_in_tree)\n",
    "        spanlists = self._get_srl_spans(grid)\n",
    "        if self._srl_includes_roleset:\n",
    "            predicates = self._get_column(grid, self._colmap['srl']+1)\n",
    "            rolesets = self._get_column(grid, self._colmap['srl'])\n",
    "        else:\n",
    "            predicates = self._get_column(grid, self._colmap['srl'])\n",
    "            rolesets = [None] * len(predicates)\n",
    "\n",
    "        instances = ConllSRLInstanceList(tree)\n",
    "        for wordnum, predicate in enumerate(predicates):\n",
    "            if predicate == '-': continue\n",
    "            for spanlist in spanlists:\n",
    "                for (start, end), tag in spanlist:\n",
    "                    if wordnum in range(start,end) and tag in ('V', 'C-V'):\n",
    "                        break\n",
    "                else: continue\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError('No srl column found for %r' % predicate)\n",
    "            instances.append(ConllSRLInstance(tree, wordnum, predicate,\n",
    "                                              rolesets[wordnum], spanlist))\n",
    "\n",
    "        return instances\n",
    "\n",
    "    def _require(self, *columntypes):\n",
    "        for columntype in columntypes:\n",
    "            if columntype not in self._colmap:\n",
    "                raise ValueError('This corpus does not contain a %s '\n",
    "                                 'column.' % columntype)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_column(grid, column_index):\n",
    "        return [grid[i][column_index] for i in range(len(grid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получаем devset.txt и testset.txt (converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def prepare(dataset):\n",
    "    factrueval_dev_tokens = dict()\n",
    "    factrueval_dev_tokens_list = []\n",
    "    factrueval_dev_spans = dict()\n",
    "    factrueval_dev_objects = dict()\n",
    "    for file in os.listdir('./'+ dataset + '/'):\n",
    "        if file.endswith('tokens'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                lines = file_obj.readlines()\n",
    "                tokens = [line.rstrip().split() for line in lines if line.rstrip().split() != []]\n",
    "                for token in tokens:\n",
    "                    factrueval_dev_tokens[int(token[0])] = token[1:]\n",
    "                tokens = [line.rstrip().split() for line in lines]\n",
    "                for token in tokens:\n",
    "                    factrueval_dev_tokens_list.append(token)\n",
    "\n",
    "        if file.endswith('spans'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                spans = [line.rstrip().split() for line in file_obj.readlines() if line.rstrip().split() != []]\n",
    "                for span in spans:\n",
    "                    factrueval_dev_spans[span[0]] = span[1:]\n",
    "\n",
    "        if file.endswith('objects'):\n",
    "            with open('./' + dataset + '/' + file, 'r+', encoding='utf-8') as file_obj:\n",
    "                objects = [line.rstrip().split('#')[0].split() for line in file_obj.readlines() if line.rstrip().split() != []]\n",
    "                for obj in objects:\n",
    "                    factrueval_dev_objects[obj[0]] = obj[1:]\n",
    "\n",
    "    all_ne = []\n",
    "    for key, value in factrueval_dev_objects.items():\n",
    "        spans = value[1:]\n",
    "        ne = value[0]\n",
    "        all_tokens = []\n",
    "        for span in spans:\n",
    "            span_obj = factrueval_dev_spans[span]\n",
    "            token = int(span_obj[3])\n",
    "            num_of_tokens = int(span_obj[4])\n",
    "            for i in range(num_of_tokens):\n",
    "                all_tokens.append(token + i)\n",
    "        all_ne.append([ne, sorted(all_tokens)])\n",
    "\n",
    "    for ne_tokens in all_ne:\n",
    "        ne = ne_tokens[0]\n",
    "        token = ne_tokens[1]\n",
    "        for i in range(len(token)):\n",
    "            if token[i] in factrueval_dev_tokens.keys():\n",
    "                if len(token) == 1:\n",
    "                    factrueval_dev_tokens[token[i]].append(\"S-\" + ne)\n",
    "                elif (i == 0 and token[i + 1] - token[i] > 1) or (i == len(token) - 1 and token[i] - token[i - 1] > 1) or (token[i] - token[i - 1] > 1 and token[i + 1] - token[i] > 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"S-\" + ne)\n",
    "                elif (i == 0  and token[i + 1] - token[i] == 1) or (i != len(token) - 1 and token[i] - token[i - 1] > 1 and token[i + 1] - token[i] == 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"B-\" + ne)\n",
    "                elif (i == len(token) - 1 and token[i] - token[i - 1] == 1) or (i != 0 and token[i] - token[i - 1] == 1 and token[i + 1] - token[i] > 1):\n",
    "                    factrueval_dev_tokens[token[i]].append(\"E-\" + ne)\n",
    "                else: \n",
    "                    factrueval_dev_tokens[token[i]].append(\"I-\" + ne)\n",
    "\n",
    "    for i in range(len(factrueval_dev_tokens_list)):\n",
    "        if factrueval_dev_tokens_list[i] == []:\n",
    "            continue\n",
    "        number_of_token = factrueval_dev_tokens_list[i][0]\n",
    "        if int(number_of_token) in factrueval_dev_tokens.keys() and len(factrueval_dev_tokens[int(number_of_token)]) >= 4:\n",
    "            ne = factrueval_dev_tokens[int(number_of_token)][3]\n",
    "            factrueval_dev_tokens_list[i].append(ne)\n",
    "        else:\n",
    "            factrueval_dev_tokens_list[i].append(\"O\")\n",
    "\n",
    "    final = []\n",
    "    for el in factrueval_dev_tokens_list:\n",
    "        if el == []:\n",
    "            final.append(el)\n",
    "        else:\n",
    "            final.append([el[3], el[1], el[2], el[4]])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = prepare('devset')\n",
    "with open('./devset.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(\"-DOCSTART- O\\n\")\n",
    "    for line in devset:\n",
    "        if line == []:\n",
    "            file.write(\"\\n\")\n",
    "        else:\n",
    "            file.write(\"{} {} {} {}\\n\".format(*line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = prepare('testset')\n",
    "with open('./testset.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(\"-DOCSTART- O\\n\")\n",
    "    for line in testset:\n",
    "        if line == []:\n",
    "            file.write(\"\\n\")\n",
    "        else:\n",
    "            file.write(\"{} {} {} {}\\n\".format(*line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#from generator import Generator\n",
    "#from corpus import ConllCorpusReaderX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factrueval_devset = ConllCorpusReaderX('./',\n",
    "                                       fileids='devset.txt', \n",
    "                                       columntypes=['words', 'offset', 'len', 'ne'])\n",
    "\n",
    "factrueval_testset = ConllCorpusReaderX('./', \n",
    "                                        fileids='testset.txt', \n",
    "                                        columntypes=['words', 'offset', 'len', 'ne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factrueval_devset.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factrueval_devset.get_ne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(column_types=['WORD'], context_len=2)\n",
    "\n",
    "Y_train = [el[1] for el in factrueval_devset.get_ne()]\n",
    "Y_test = [el[1] for el in factrueval_testset.get_ne()] \n",
    "\n",
    "X_train = gen.fit_transform([[el] for el in factrueval_devset.words()], \n",
    "                            Y_train, \n",
    "                            path=TRAINSET_PATH)\n",
    "X_test = gen.transform([[el] for el in factrueval_testset.words()], \n",
    "                       path=TESTSET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Избавляет данные от случаев O : O #\n",
    "def clean(Y_pred, Y_test):\n",
    "    Y_pred = np.array(Y_pred)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    Y_pred_i = np.array([Y_pred != 'O'])\n",
    "    Y_test_i = np.array([Y_test != 'O'])\n",
    "\n",
    "    indexes = (Y_pred_i | Y_test_i).reshape(Y_pred.shape)\n",
    "\n",
    "    Y_pred_fixed = Y_pred[indexes]\n",
    "    Y_test_fixed = Y_test[indexes]\n",
    "    return Y_pred_fixed, Y_test_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(clf=LogisticRegression()):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    Y_pred_c, Y_test_c = clean(Y_pred, Y_test)\n",
    "\n",
    "    def get_el(el):\n",
    "        if el == \"O\":\n",
    "            return el\n",
    "        else:\n",
    "            return el[2:]\n",
    "\n",
    "    Y_pred_c_light = [get_el(el) for el in Y_pred_c]\n",
    "    Y_test_c_light = [get_el(el) for el in Y_test_c]\n",
    "\n",
    "    # Strict evaluation #\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"# Strict evaluation #\")\n",
    "    counter = Counter(Y_test_c)\n",
    "    labels = list(counter.keys())\n",
    "    labels.remove(\"O\")\n",
    "    results = f1_score(Y_test_c, Y_pred_c, average=None, labels=labels)\n",
    "    for a, b in zip(labels, results):\n",
    "        print('F1 for {} == {}, with {} entities'.format(a, b, counter[a]))\n",
    "\n",
    "    print(\"Weighted Score:\", f1_score(Y_test_c, Y_pred_c, average=\"weighted\", labels=list(counter.keys())))    \n",
    "\n",
    "    # Not strict evaluation #    \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"# Not strict evaluation #\")    \n",
    "    light_counter = Counter(Y_test_c_light)\n",
    "    light_labels = list(light_counter.keys())\n",
    "    light_labels.remove(\"O\")\n",
    "    print(light_counter)\n",
    "    light_results = f1_score(Y_test_c_light, Y_pred_c_light, average=None, labels=light_labels)\n",
    "    for a, b in zip(light_labels, light_results):\n",
    "        print('F1 for {} == {}, with {} entities'.format(a, b, light_counter[a]))\n",
    "\n",
    "    print(\"Weighted Score:\", f1_score(Y_test_c_light, Y_pred_c_light, average=\"weighted\", labels=light_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline(LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One classifier for all classes (with prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_c, Y_test_c = clean(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strict evaluation of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(Y_test_c)\n",
    "labels = list(counter.keys())\n",
    "labels.remove(\"O\")\n",
    "results = f1_score(Y_test_c, Y_pred_c, average=None, labels=labels)\n",
    "for a, b in zip(labels, results):\n",
    "    print('F1 for {} == {}, with {} entities'.format(a, b, counter[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f1_score(Y_test_c, Y_pred_c, average=\"weighted\", labels=list(counter.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not strict evaluation of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_el(el):\n",
    "    if el == \"O\":\n",
    "        return el\n",
    "    else:\n",
    "        return el[2:]\n",
    "    \n",
    "Y_pred_c_light = [get_el(el) for el in Y_pred_c]\n",
    "Y_test_c_light = [get_el(el) for el in Y_test_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_counter = Counter(Y_test_c_light)\n",
    "light_labels = list(light_counter.keys())\n",
    "light_labels.remove(\"O\")\n",
    "print(light_counter)\n",
    "light_results = f1_score(Y_test_c_light, Y_pred_c_light, average=None, labels=light_labels)\n",
    "for a, b in zip(light_labels, light_results):\n",
    "    print('F1 for {} == {}, with {} entities'.format(a, b, light_counter[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f1_score(Y_test_c_light, Y_pred_c_light, average=\"weighted\", labels=light_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One classifier for all classes (without prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_el(el):\n",
    "    if el == \"O\":\n",
    "        return el\n",
    "    else:\n",
    "        return el[2:]\n",
    "\n",
    "Y_train = [get_el(el[1]) for el in factrueval_devset.get_ne()]\n",
    "Y_test = [get_el(el[1]) for el in factrueval_testset.get_ne()] \n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "Y_pred_c, Y_test_c = clean(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not strict evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_counter = Counter(Y_test_c)\n",
    "light_labels = list(light_counter.keys())\n",
    "print(light_counter)\n",
    "light_results = f1_score(Y_test_c, Y_pred_c, average=None, labels=light_labels)\n",
    "for a, b in zip(light_labels, light_results):\n",
    "    print('F1 for {} == {}, with {} entities'.format(a, b, light_counter[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(Y_test_c, Y_pred_c, average=\"weighted\", labels=light_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different classifiers for different classes (without prefixes and with prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diff_classes(template, prefixes=False):\n",
    "    def get_only(el):\n",
    "        if (el[2:] == template):\n",
    "            return el[2:]\n",
    "        else:\n",
    "            return \"O\"\n",
    "        \n",
    "    Y_train = [get_only(el[1]) for el in factrueval_devset.get_ne()]\n",
    "    Y_test = [get_only(el[1]) for el in factrueval_testset.get_ne()] \n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "\n",
    "    Y_pred_c, Y_test_c = clean(Y_pred, Y_test)\n",
    "    \n",
    "    light_counter = Counter(Y_test_c)\n",
    "    light_counter_2 = Counter(Y_pred_c)\n",
    "    labels = list(light_counter.keys())\n",
    "    labels.remove(\"O\")\n",
    "    print(labels)\n",
    "    light_result = f1_score(Y_test_c, Y_pred_c, average=\"binary\", pos_label=template)\n",
    "    print('F1 for {} == {}, with {} entities'.format(template, light_result, light_counter[template]))\n",
    "        \n",
    "    return light_result, light_counter[template]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1, weight1 = run_diff_classes(\"Person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2, weight2 = run_diff_classes(\"Org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3, weight3 = run_diff_classes(\"LocOrg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4, weight4 = run_diff_classes(\"Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_weight = weight1 + weight2 + weight3 + weight4\n",
    "total_result = (result1 * weight1 + result2 * weight2 + result3 * weight3 + result4 * weight4) / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(total_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
